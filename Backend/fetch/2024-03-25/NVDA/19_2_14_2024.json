{
    "title": "Nvidia previews on-device chatbot, in approach favored by Apple",
    "date": "2/14/2024",
    "url": "https://9to5mac.com/2024/02/14/nvidia-on-device-chatbot/",
    "text": "One of Apple\u2019s apparent AI ambitions is to enable an on-device chatbot which can run on iPhones, rather than using servers to do the processing. This would be capable of accessing data stored on your iPhone, as well as boosting privacy \u2026 Nvidia has taken the same approach, with a chatbot that runs on a Windows PC \u2013 naturally, one equipped with one of the company\u2019s own high-end GPUs. An Apple research paper published late last year seemed to point to the company enabling an on-device chatbot to run on an iPhone. The paper is about minimizing the amount of data which needs to be transferred from flash storage to RAM. LLMs is the generic term for AI chat systems that have been trained on large amounts of text. This approach enables LLMs to run up to 25 times faster on devices with limited RAM. While the paper doesn\u2019t reveal any specifics about Apple\u2019s plans, there have been increasing indications that iOS 18 is the point at which the company will finally launch a far more powerful version of Siri, using the type of generative AI tech dubbed AppleGPT. The next version of iOS, set to be unveiled at WWDC in June, is said to be the biggest update in the company\u2019s history. Nvidia has already created a demo version of one on-device chatbot, albeit one which requires a Windows PC with one of the company\u2019s own premium GPU cards. Chat With RTX is a demo app that lets you personalize a GPT large language model (LLM) connected to your own content\u2014docs, notes, videos, or other data. Leveraging retrieval-augmented generation (RAG), TensorRT-LLM, and RTX acceleration, you can query a custom chatbot to quickly get contextually relevant answers. And because it all runs locally on your Windows RTX PC or workstation, you\u2019ll get fast and secure results [\u2026] Chat with RTX supports various file formats, including text, pdf, doc/docx, and xml. Simply point the application at the folder containing your files and it\u2019ll load them into the library in a matter of seconds. Additionally, you can provide the url of a YouTube playlist and the app will load the transcriptions of the videos in the playlist, enabling you to query the content they cover. Your Windows machine will need an NVIDIA GeForce RTX 30 or 40 Series GPU, or an RTX Ampere or Ada Generation GPU with at least 8GB of VRAM. In other AI news, ChatGPT is getting a memory \u2013 at least, for a small number of users. One of the limitations of ChatGPT to date has been that each session runs in isolation \u2013 that is, it won\u2019t remember anything you did in previous sessions. OpenAI is now testing a version which can remember certain data and preferences, for a more coherent experience over time. We\u2019re testing memory with ChatGPT. Remembering things you discuss across all chats saves you from having to repeat information and makes future conversations more helpful. You\u2019re in control of ChatGPT\u2019s memory. You can explicitly tell it to remember something, ask it what it remembers, and tell it to forget conversationally or through settings. You can also turn it off entirely [\u2026] ChatGPT\u2019s memory will get better the more you use it and you\u2019ll start to notice the improvements over time. For example: The company says that it\u2019s being trialled with a small number of users this week, and that it will share wider rollout plans once it has assessed the results. FTC: We use income earning auto affiliate links. More. Check out 9to5Mac on YouTube for more Apple news: Nvidia is best known as a designer of GPUs (Grap\u2026 Ben Lovejoy is a British technology writer and EU Editor for 9to5Mac. He\u2019s known for his op-eds and diary pieces, exploring his experience of Apple products over time, for a more rounded review. He also writes fiction, with two technothriller novels, a couple of SF shorts and a rom-com! "
}