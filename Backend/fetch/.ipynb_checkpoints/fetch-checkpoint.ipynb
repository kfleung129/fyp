{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ef6c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\endyj\\anaconda3\\envs\\py38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import string\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246b796",
   "metadata": {},
   "source": [
    "# Google Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dcd7fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "def get_useragent():\n",
    "    return random.choice(_useragent_list)\n",
    "\n",
    "_useragent_list = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36 Edg/111.0.1661.62',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0'\n",
    "]\n",
    "\n",
    "request_timeout = 10\n",
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:49.0) Gecko/20100101 Firefox/49.0',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Cache-Control': 'private, max-age=0',\n",
    "}\n",
    "html_pattern = \"<(?:\\\"[^\\\"]*\\\"['\\\"]*|'[^']*'['\\\"]*|[^'\\\">])+>\"\n",
    "TODAY = datetime.date.today()\n",
    "#TODAY = '2024-03-22'\n",
    "\n",
    "#search variables\n",
    "from_date = '1/1/2024'\n",
    "to_date = '1/3/2024'\n",
    "total_news = 20\n",
    "sleep_interval = 20\n",
    "stock_name = 'NVDA'\n",
    "\n",
    "#NLP variables\n",
    "model_id = \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ceba417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"googlesearch is a Python library for searching Google, easily.\"\"\"\n",
    "def get_headers(): \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:49.0) Gecko/20100101 Firefox/49.0',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Cache-Control': 'private, max-age=0',\n",
    "    }\n",
    "\n",
    "    return headers\n",
    "\n",
    "def _req(term, results, lang, start, proxies, timeout, from_date, to_date):\n",
    "    resp = get(\n",
    "        url=\"https://www.google.com/search\",\n",
    "        headers={\n",
    "            \"User-Agent\": get_useragent()\n",
    "        },\n",
    "        params={\n",
    "            \"q\": term,\n",
    "            \"num\": results,  # Prevents multiple requests\n",
    "            \"hl\": lang,\n",
    "            \"start\": start,\n",
    "            \"lr\": \"lang_en\",\n",
    "            \"tbm\": 'nws',\n",
    "            \"tbs\": f'sbd:1,cdr:1,cd_min:{from_date},cd_max:{to_date}'\n",
    "            #\"tbs\": \"qdr:d\"\n",
    "            #\"tbs\": f\"sbd:1,lr:lang_en,cdr:1,cd_min:1/1/{from_year},cd_max:{to_year}\"\n",
    "        },\n",
    "        proxies=proxies,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    return resp\n",
    "\n",
    "def search(term, num_results=10, lang=\"en\", proxy=None, advanced=False, sleep_interval=0, timeout=5, from_date='1/1/2020', to_date='1/1/2020'):\n",
    "    \"\"\"Search the Google search engine\"\"\"\n",
    "\n",
    "    escaped_term = urllib.parse.quote_plus(term) # make 'site:xxx.xxx.xxx ' works.\n",
    "\n",
    "    # Proxy\n",
    "    proxies = None\n",
    "    if proxy:\n",
    "        if proxy.startswith(\"https\"):\n",
    "            proxies = {\"https\": proxy}\n",
    "        else:\n",
    "            proxies = {\"http\": proxy}\n",
    "\n",
    "    # Fetch\n",
    "    start = 0\n",
    "    res = []\n",
    "    while start < num_results:\n",
    "        # Send request\n",
    "        resp = _req(escaped_term, num_results - start,\n",
    "                    lang, start, proxies, timeout, from_date, to_date)\n",
    "        # Parse\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        result_block = soup.find_all(\"div\", attrs={\"class\": \"SoaBEf\"})\n",
    "\n",
    "        if len(result_block)==0:\n",
    "            break\n",
    "\n",
    "        for result in result_block:\n",
    "            # Find url, title, description\n",
    "            url = result.find(\"a\", href=True)\n",
    "            title = result.find(\"div\", attrs={\"class\": \"n0jPhd ynAwRc MBeuO nDgy9d\"})\n",
    "            date = result.find(\"div\", attrs={\"class\": \"OSrXXb rbYSKb LfVVr\"})\n",
    "            obj = {}\n",
    "            obj[\"title\"] = None\n",
    "            obj[\"date\"] = None\n",
    "            obj[\"url\"] = None\n",
    "            start += 1\n",
    "\n",
    "            if title:\n",
    "                obj[\"title\"] = title.text\n",
    "            if date:\n",
    "                obj[\"date\"] = from_date\n",
    "            if url:\n",
    "                obj[\"url\"] = url[\"href\"]\n",
    "                res.append(obj)\n",
    "                        \n",
    "        sleep(sleep_interval)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fde9b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_news_urls(q, total, from_date, to_date, sleep_interval=0):\n",
    "    urls = []\n",
    "    urls = search(q, num_results=total, from_date=from_date, to_date=to_date, sleep_interval=sleep_interval)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b254fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_news_urls(q, total, from_date, to_date, temp_save=False):\n",
    "    dir_name = str(TODAY)\n",
    "    if dir_name not in os.listdir():\n",
    "        cwd = os.getcwd()\n",
    "        os.mkdir(f'{cwd}/{dir_name}')\n",
    "        \n",
    "    urls = get_google_news_urls(q, total, from_date, to_date, sleep_interval = 10)\n",
    "    # temp\n",
    "    if temp_save:\n",
    "        # replace date format to file savable characters\n",
    "        filename = f'{query}_{from_date.replace(\"/\", \"_\")}.json'\n",
    "        f = open(f'./{TODAY}/{filename}', 'w')\n",
    "        f.write(json.dumps(json_object, indent = 4))\n",
    "        f.close()\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f42a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_data(url):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        article = requests.get(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            timeout=request_timeout\n",
    "        )\n",
    "        status_code = article.status_code\n",
    "        if status_code >= 400:\n",
    "            f = open(f'./{TODAY}/ERROR_LOG.txt', 'a')\n",
    "            f.write(f'[{time.asctime(time.localtime())}] Code {status_code}: {url}\\n')\n",
    "            f.close()\n",
    "            return None\n",
    "        soup = bs(article.content, \"html.parser\")\n",
    "        article_body = soup.find(\"body\")\n",
    "        paragraphs = article_body.find_all(\"p\")\n",
    "        #sleep(1)\n",
    "        if paragraphs is not None:\n",
    "            for p in paragraphs:\n",
    "                text += re.sub(html_pattern, '', p.text).strip() + ' '  \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        msg = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef8e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(q, json_object, date):\n",
    "    total_count = 0\n",
    "    for data in json_object:  \n",
    "        try:\n",
    "            data['text'] = get_news_data(data['url'])\n",
    "            if data is None:\n",
    "                continue\n",
    "            if q not in os.listdir(f'./{TODAY}'):\n",
    "                cwd = os.getcwd()\n",
    "                os.mkdir(f'{cwd}/{TODAY}/{q}')\n",
    "            total_count += 1\n",
    "            file_error_symbols = []\n",
    "            filename = f'{total_count}_{date.replace(\"/\", \"_\")}'\n",
    "            f = open(f'./{TODAY}/{q}//{filename}.json', \"w\")\n",
    "            f.write(json.dumps(data, indent = 4))\n",
    "            f.close()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "    print(f'Saved {total_count} news data in {date}')\n",
    "    #print(os.system(\"npx prettier -w ./dataset/*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c56ef85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_from_file(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fdadc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_from_file(filename):\n",
    "    keywords = []\n",
    "    f = open(filename, \"r\")\n",
    "    for keyword in f:\n",
    "        keywords.append(keyword)\n",
    "    f.close()\n",
    "    return list(map(lambda keyword: keyword.strip(), keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "349e49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_save_dataset(q, urls):\n",
    "    start = time.time()\n",
    "    for url in urls:\n",
    "        print(urls)\n",
    "        save_dataset(q, urls[year], year)\n",
    "    #Parallel(n_jobs = -1)(delayed(save_dataset)(q, urls[year], year) for year in urls)\n",
    "    end = time.time()\n",
    "    print('{:.2f} seconds used'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deaf2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run all functions to save news data\n",
    "def scrap_news_data(total, from_date, to_date, keywords_filename=\"keywords.txt\"):\n",
    "    #count used time\n",
    "    start = time.time()\n",
    "    #step 1 - load keywords file\n",
    "    keywords = get_keywords_from_file(keywords_filename)\n",
    "    for query in keywords:\n",
    "        print(f'Scrapping {query} ...')\n",
    "        #step 2 - save urls to json file\n",
    "        save_news_urls(query, total, from_date, to_date)\n",
    "        #step 3 - get urls object from previous saved json file\n",
    "        urls = get_urls_from_file(f'./{TODAY}/{query}.json')\n",
    "        #step 4 - web scrapping news data from specific year of urls & save into directory seperately\n",
    "        save_dataset(query, urls, from_date)\n",
    "        #parallel_save_dataset(query, urls, date)\n",
    "        #save_dataset(query, urls)\n",
    "        \n",
    "    end = time.time()\n",
    "    print('[DONE] {:.2f} seconds used'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84a37454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv(q, json_obj, index):\n",
    "    text = None\n",
    "    title = None\n",
    "    date = None\n",
    "    url = json_obj['url']\n",
    "    if 'date' in json_obj.keys():\n",
    "        date = json_obj['date']\n",
    "    if json_obj['text'] is not None:\n",
    "        text = re.sub(r'[,.\\'\"|]', '', json_obj['text'])\n",
    "    if 'title' in json_obj.keys() and json_obj['title'] != None:\n",
    "        title = re.sub(r'[,.\\'\"|]', '', json_obj['title'])\n",
    "    if text != '' and text != None:\n",
    "        text = ' '.join(text.encode('utf-8').decode().split())\n",
    "    if title != '' and title != None:\n",
    "        title = ' '.join(title.split()).strip() \n",
    "    f = open(f'./{TODAY}/{q}.csv', 'a', encoding='utf-8')\n",
    "    f.write(f'{index},\\\"{title}\\\",\\\"{date}\\\",\\\"{url}\\\",\\\"{text}\\\"\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49f6fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date_time_format(date):\n",
    "    date = date.split('/')\n",
    "    date = list(map(lambda x: int(x), date))\n",
    "    return datetime.datetime(date[2], date[1], date[0])\n",
    "\n",
    "def to_google_param_format(date):\n",
    "    return f'{date.month}/{date.day}/{date.year}'\n",
    "\n",
    "def save_json_object(filename, json_object):\n",
    "    f = open(f'./{TODAY}/{filename}', 'w')\n",
    "    f.write(json.dumps(json_object, indent = 4))\n",
    "    f.close()\n",
    "    \n",
    "def get_date_range_list(from_date, to_date):\n",
    "    from_date_time = to_date_time_format(from_date)\n",
    "    to_date_time = to_date_time_format(to_date)\n",
    "    day_difference = (to_date_time - from_date_time).days\n",
    "    datetime_list = [from_date_time + datetime.timedelta(days=x) for x in range(day_difference + 1)]\n",
    "    date_range_list = list(map(lambda x: to_google_param_format(x), datetime_list))\n",
    "    return date_range_list\n",
    "    \n",
    "def parallel_save_dataset(query, from_date, to_date):\n",
    "    # count used time\n",
    "    start = time.time()\n",
    "    # JSON object\n",
    "    json_object = dict()\n",
    "    # get from date & to date difference days\n",
    "    date_range_list = get_date_range_list(from_date, to_date)\n",
    "    # iterate through date range list\n",
    "    for date in date_range_list:\n",
    "        urls = save_news_urls(query, total_news, date, date)\n",
    "        json_object[date] = urls\n",
    "        \n",
    "    # replace date format to file savable characters\n",
    "    filename = f'{query}.json'\n",
    "    # temporary save of the scrapped urls\n",
    "    save_json_object(filename, json_object)\n",
    "    # parallel computing to save news data\n",
    "    Parallel(n_jobs = -1)(delayed(save_dataset)(query, json_object[date], date) for date in json_object)\n",
    "    #for date in json_object:\n",
    "    #    save_dataset(query, json_object[date], date)\n",
    "    # convert all json file in single csv file\n",
    "    all_json_to_csv(stock_name)\n",
    "    print(f'{filename}: [DONE]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0761ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_json_to_csv(stock_name):\n",
    "    cols = \"title,date,url,text\\n\"\n",
    "    keywords = get_keywords_from_file('keywords.txt')\n",
    "    for keyword in keywords:\n",
    "        f = open(f'./{TODAY}/{stock_name}.csv', 'w')\n",
    "        f.write(cols)\n",
    "        f.close()\n",
    "        for index, filename in enumerate(os.listdir(f'./{TODAY}/{stock_name}')):\n",
    "            if '.json' not in filename:\n",
    "                continue\n",
    "            json_file = open(f'./{TODAY}/{stock_name}/{filename}', 'r')\n",
    "            json_obj = json.load(json_file)\n",
    "            json_file.close()\n",
    "            json_to_csv(stock_name, json_obj, index)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95b07911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_text(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    special_characters = \"!@#$%^&*()-+?_=,<>\\\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_words = word_tokenize(text.lower())\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_words]\n",
    "    preprocessed_words = [word for word in lemmatized_words if word not in stop_words]\n",
    "    preprocessed_words = [word for word in lemmatized_words if word not in special_characters]\n",
    "    preprocessed_text = \" \".join(preprocessed_words)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0441cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_sentiment_score(news_df):\n",
    "    sentiment_obj = dict()\n",
    "    sentiment_score_df = []\n",
    "    news_df_length = len(news_df)\n",
    "    \n",
    "    for i in range(news_df_length):\n",
    "        data = news_df.iloc[i]\n",
    "        date = data['date']\n",
    "        title = data['title']\n",
    "        text = data['text']\n",
    "        preprocessed_text = get_preprocessed_text(text)\n",
    "        sentiment_result = sentiment_pipe(preprocessed_text)[0]\n",
    "        score = sentiment_result['score']\n",
    "        label = sentiment_result['label']\n",
    "        \n",
    "        if date not in sentiment_obj.keys():\n",
    "            sentiment_obj[date] = dict()\n",
    "            sentiment_obj[date]['score'] = 0\n",
    "            sentiment_obj[date]['count'] = 0\n",
    "        if label != 'neutral': \n",
    "            if label == 'positive':\n",
    "                sentiment_obj[date]['score'] += score\n",
    "            else:\n",
    "                sentiment_obj[date]['score'] -= score\n",
    "            sentiment_obj[date]['count'] += 1\n",
    "    \n",
    "    for date in sentiment_obj:\n",
    "        date_list = list(map(lambda x: int(x), date.split('/')))\n",
    "        datetime_obj = datetime.datetime(date_list[2], date_list[0], date_list[1])\n",
    "        score = sentiment_obj[date]['score']\n",
    "        count = sentiment_obj[date]['count']\n",
    "        if count > 0:\n",
    "            item = [datetime_obj,  score / count]\n",
    "            sentiment_score_df.append(item)\n",
    "        \n",
    "    sentiment_score_df = sorted(sentiment_score_df, key=lambda x: x[0])\n",
    "    sentiment_score_df = list(map(lambda x: [str(x[0].date()), x[1]], sentiment_score_df))\n",
    "    sentiment_score_df = pd.DataFrame(sentiment_score_df, columns =['Date', 'Score'], dtype = float) \n",
    "    return sentiment_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c204d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movement_accuracy(stock_df, sentiment_df):\n",
    "    stock_available_dates = stock_df['Date'].values\n",
    "    date_length = len(stock_available_dates)\n",
    "    stock_val_list = stock_df['Adj Close'].values\n",
    "    sentiment_date_list = sentiment_df['Date'].values\n",
    "    \n",
    "    real_label_list = []\n",
    "    predict_label_list = []\n",
    "    \n",
    "    for i in range(len(sentiment_date_list)):\n",
    "        if sentiment_date_list[i] not in stock_available_dates:\n",
    "            sentiment_df = sentiment_df.drop(i)\n",
    "    \n",
    "    sentiment_score_list = sentiment_df['Score'].values\n",
    "\n",
    "    for i in range(1, date_length):\n",
    "        real_diff = stock_val_list[i] - stock_val_list[i-1]\n",
    "        sentiment_diff = sentiment_score_list[i] - sentiment_score_list[i-1]\n",
    "        \n",
    "        if real_diff > 0:\n",
    "            label = 'up'\n",
    "        else:\n",
    "            label = 'drop'\n",
    "            \n",
    "        real_label_list.append(label)\n",
    "        \n",
    "        if sentiment_diff > 0:\n",
    "            label = 'up'\n",
    "        else:\n",
    "            label = 'drop'\n",
    "            \n",
    "        predict_label_list.append(label)\n",
    "        \n",
    "    # compare the movement accuracy\n",
    "    label_list_length = len(real_label_list)\n",
    "    total = 0\n",
    "    for i in range(label_list_length):\n",
    "        if real_label_list[i] == predict_label_list[i]:\n",
    "            total += 1\n",
    "    accuracy = total / date_length\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "42f02735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(index_arr, score_arr):\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    ax.plot(index_arr, score_arr)\n",
    "    ax.set_title(f'AAPL Sentiment Score')\n",
    "    ax.set_xlabel(f'Date')\n",
    "    ax.set_ylabel(f'Score')\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(base=10))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(base=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01fc43d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 8 news data in 1/1/2024\n",
      "Saved 12 news data in 1/2/2024\n",
      "Saved 14 news data in 1/3/2024\n",
      "Saved 20 news data in 1/4/2024\n",
      "Saved 20 news data in 1/5/2024\n",
      "Saved 6 news data in 1/6/2024\n",
      "Saved 5 news data in 1/7/2024\n",
      "Saved 20 news data in 1/8/2024\n",
      "Saved 20 news data in 1/9/2024\n",
      "Saved 20 news data in 1/10/2024\n",
      "Saved 20 news data in 1/11/2024\n",
      "Saved 20 news data in 1/12/2024\n",
      "Saved 17 news data in 1/13/2024\n",
      "Saved 12 news data in 1/14/2024\n",
      "Saved 20 news data in 1/15/2024\n",
      "Saved 20 news data in 1/16/2024\n",
      "Saved 20 news data in 1/17/2024\n",
      "Saved 20 news data in 1/18/2024\n",
      "Saved 20 news data in 1/19/2024\n",
      "Saved 11 news data in 1/20/2024\n",
      "Saved 5 news data in 1/21/2024\n",
      "Saved 20 news data in 1/22/2024\n",
      "Saved 20 news data in 1/23/2024\n",
      "Saved 20 news data in 1/24/2024\n",
      "Saved 20 news data in 1/25/2024\n",
      "Saved 20 news data in 1/26/2024\n",
      "Saved 19 news data in 1/27/2024\n",
      "Saved 20 news data in 1/28/2024\n",
      "Saved 20 news data in 1/29/2024\n",
      "Saved 20 news data in 1/30/2024\n",
      "Saved 20 news data in 1/31/2024\n",
      "Saved 20 news data in 2/1/2024\n",
      "Saved 20 news data in 2/2/2024\n",
      "Saved 10 news data in 2/3/2024\n",
      "Saved 16 news data in 2/4/2024\n",
      "Saved 20 news data in 2/5/2024\n",
      "Saved 20 news data in 2/6/2024\n",
      "Saved 20 news data in 2/7/2024\n",
      "Saved 20 news data in 2/8/2024\n",
      "Saved 20 news data in 2/9/2024\n",
      "Saved 14 news data in 2/10/2024\n",
      "Saved 11 news data in 2/11/2024\n",
      "Saved 20 news data in 2/12/2024\n",
      "Saved 20 news data in 2/13/2024\n",
      "Saved 20 news data in 2/14/2024\n",
      "Saved 20 news data in 2/15/2024\n",
      "Saved 20 news data in 2/16/2024\n",
      "Saved 10 news data in 2/17/2024\n",
      "Saved 8 news data in 2/18/2024\n",
      "Saved 20 news data in 2/19/2024\n",
      "Saved 20 news data in 2/20/2024\n",
      "Saved 20 news data in 2/21/2024\n",
      "Saved 20 news data in 2/22/2024\n",
      "Saved 20 news data in 2/23/2024\n",
      "Saved 14 news data in 2/24/2024\n",
      "Saved 15 news data in 2/25/2024\n",
      "Saved 20 news data in 2/26/2024\n",
      "Saved 20 news data in 2/27/2024\n",
      "Saved 20 news data in 2/28/2024\n",
      "Saved 20 news data in 2/29/2024\n",
      "Saved 20 news data in 3/1/2024\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系統找不到指定的路徑。: './2024-03-25/AAPL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mparallel_save_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_date\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 41\u001b[0m, in \u001b[0;36mparallel_save_dataset\u001b[1;34m(query, from_date, to_date)\u001b[0m\n\u001b[0;32m     39\u001b[0m     save_dataset(query, json_object[date], date)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# convert all json file in single csv file\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[43mall_json_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: [DONE]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m, in \u001b[0;36mall_json_to_csv\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(cols)\n\u001b[0;32m      7\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, filename \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mTODAY\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkeyword\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m filename:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系統找不到指定的路徑。: './2024-03-25/AAPL'"
     ]
    }
   ],
   "source": [
    "parallel_save_dataset(stock_name, from_date, to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28cfe03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Microsoft nears Apple in race for worlds most ...</td>\n",
       "      <td>1/10/2024</td>\n",
       "      <td>https://seekingalpha.com/news/4054010-microsof...</td>\n",
       "      <td>Prykhodov Microsoft (NASDAQ:MSFT) could soon o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Microsoft Tops Apple as Worlds Most Valuable C...</td>\n",
       "      <td>1/11/2024</td>\n",
       "      <td>https://www.pymnts.com/news/investment-tracker...</td>\n",
       "      <td>Microsoft’s AI investments have helped it surp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Microsoft passes Apple as most valuable public...</td>\n",
       "      <td>1/12/2024</td>\n",
       "      <td>https://seekingalpha.com/news/4054867-microsof...</td>\n",
       "      <td>Yuriy Komarov Microsoft (NASDAQ:MSFT) overtook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Move over Apple: Microsoft is now the most val...</td>\n",
       "      <td>1/13/2024</td>\n",
       "      <td>https://www.ctvnews.ca/business/move-over-appl...</td>\n",
       "      <td>Microsoft is back on top After trailing behind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Microsoft has overtaken Apple as the world’s m...</td>\n",
       "      <td>1/14/2024</td>\n",
       "      <td>https://www.tekedia.com/microsoft-has-overtake...</td>\n",
       "      <td>DD MM YYYY CATEGORIES PAGES DD MM YYYY FACEBOO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2/6/2024</td>\n",
       "      <td>https://www.fudzilla.com/news/58392-microsoft-...</td>\n",
       "      <td>It is worth ten times more While we might miss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>Microsofts last minute Super Bowl ad showcases...</td>\n",
       "      <td>2/7/2024</td>\n",
       "      <td>https://www.campaignlive.com/article/microsoft...</td>\n",
       "      <td>Sixty second spot will broadly launch the comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>Enjoy a Lifetime of Productivity With Microsof...</td>\n",
       "      <td>2/8/2024</td>\n",
       "      <td>https://www.tmz.com/2024/02/08/enjoy-a-lifetim...</td>\n",
       "      <td>TMZ may collect a share of sales or other comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>IPOs coming in hot for the second quarter: rep...</td>\n",
       "      <td>2/9/2024</td>\n",
       "      <td>https://seekingalpha.com/news/4065265-ipos-com...</td>\n",
       "      <td>WANAN YOSSINGKUM Initial public offerings may ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>Copilot for Finance streamlines productivity a...</td>\n",
       "      <td>3/1/2024</td>\n",
       "      <td>https://windowsreport.com/copilot-for-finance-...</td>\n",
       "      <td>Follow us Share this article Latest news The s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>905 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title       date  \\\n",
       "0     Microsoft nears Apple in race for worlds most ...  1/10/2024   \n",
       "1     Microsoft Tops Apple as Worlds Most Valuable C...  1/11/2024   \n",
       "2     Microsoft passes Apple as most valuable public...  1/12/2024   \n",
       "3     Move over Apple: Microsoft is now the most val...  1/13/2024   \n",
       "4     Microsoft has overtaken Apple as the world’s m...  1/14/2024   \n",
       "...                                                 ...        ...   \n",
       "1062                                                NaN   2/6/2024   \n",
       "1063  Microsofts last minute Super Bowl ad showcases...   2/7/2024   \n",
       "1064  Enjoy a Lifetime of Productivity With Microsof...   2/8/2024   \n",
       "1065  IPOs coming in hot for the second quarter: rep...   2/9/2024   \n",
       "1066  Copilot for Finance streamlines productivity a...   3/1/2024   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://seekingalpha.com/news/4054010-microsof...   \n",
       "1     https://www.pymnts.com/news/investment-tracker...   \n",
       "2     https://seekingalpha.com/news/4054867-microsof...   \n",
       "3     https://www.ctvnews.ca/business/move-over-appl...   \n",
       "4     https://www.tekedia.com/microsoft-has-overtake...   \n",
       "...                                                 ...   \n",
       "1062  https://www.fudzilla.com/news/58392-microsoft-...   \n",
       "1063  https://www.campaignlive.com/article/microsoft...   \n",
       "1064  https://www.tmz.com/2024/02/08/enjoy-a-lifetim...   \n",
       "1065  https://seekingalpha.com/news/4065265-ipos-com...   \n",
       "1066  https://windowsreport.com/copilot-for-finance-...   \n",
       "\n",
       "                                                   text  \n",
       "0     Prykhodov Microsoft (NASDAQ:MSFT) could soon o...  \n",
       "1     Microsoft’s AI investments have helped it surp...  \n",
       "2     Yuriy Komarov Microsoft (NASDAQ:MSFT) overtook...  \n",
       "3     Microsoft is back on top After trailing behind...  \n",
       "4     DD MM YYYY CATEGORIES PAGES DD MM YYYY FACEBOO...  \n",
       "...                                                 ...  \n",
       "1062  It is worth ten times more While we might miss...  \n",
       "1063  Sixty second spot will broadly launch the comp...  \n",
       "1064  TMZ may collect a share of sales or other comp...  \n",
       "1065  WANAN YOSSINGKUM Initial public offerings may ...  \n",
       "1066  Follow us Share this article Latest news The s...  \n",
       "\n",
       "[905 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.read_csv(f\"./{TODAY}/{stock_name}.csv\")\n",
    "news_df = news_df.mask(news_df.eq('None')).dropna(subset=['text'])\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "734e9910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "stock_df = yf.download(stock_name, start=f'2024-01-01', end=f'2024-03-01')\n",
    "stock_df = stock_df.reset_index()\n",
    "stock_df['Date'] = stock_df['Date'].apply(lambda x: str(x.date()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94a5db82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "sentiment_df = get_weighted_sentiment_score(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "76577f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df.to_csv(f'./{TODAY}/{stock_name}_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7ae37da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT Sentiment Analysis Accuracy: 0.5609756097560976\n"
     ]
    }
   ],
   "source": [
    "accuracy = get_movement_accuracy(stock_df, sentiment_df)\n",
    "print(f'{stock_name} Sentiment Analysis Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d14dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
