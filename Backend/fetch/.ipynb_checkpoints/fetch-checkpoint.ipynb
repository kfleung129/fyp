{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f3ef6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import string\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import random\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246b796",
   "metadata": {},
   "source": [
    "# Google Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9dcd7fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "def get_useragent():\n",
    "    return random.choice(_useragent_list)\n",
    "\n",
    "_useragent_list = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36 Edg/111.0.1661.62',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0'\n",
    "]\n",
    "\n",
    "request_timeout = 10\n",
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:49.0) Gecko/20100101 Firefox/49.0',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Cache-Control': 'private, max-age=0',\n",
    "}\n",
    "html_pattern = \"<(?:\\\"[^\\\"]*\\\"['\\\"]*|'[^']*'['\\\"]*|[^'\\\">])+>\"\n",
    "TODAY = datetime.date.today()\n",
    "\n",
    "#search variables\n",
    "from_date = '1/1/2024'\n",
    "to_date = '1/3/2024'\n",
    "total_news = 10\n",
    "sleep_interval = 20\n",
    "\n",
    "#NLP variables\n",
    "model_id = \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ceba417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"googlesearch is a Python library for searching Google, easily.\"\"\"\n",
    "def get_headers(): \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:49.0) Gecko/20100101 Firefox/49.0',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Cache-Control': 'private, max-age=0',\n",
    "    }\n",
    "\n",
    "    return headers\n",
    "\n",
    "def _req(term, results, lang, start, proxies, timeout, from_date, to_date):\n",
    "    resp = get(\n",
    "        url=\"https://www.google.com/search\",\n",
    "        headers={\n",
    "            \"User-Agent\": get_useragent()\n",
    "        },\n",
    "        params={\n",
    "            \"q\": term,\n",
    "            \"num\": results,  # Prevents multiple requests\n",
    "            \"hl\": lang,\n",
    "            \"start\": start,\n",
    "            \"lr\": \"lang_en\",\n",
    "            \"tbm\": 'nws',\n",
    "            \"tbs\": f'sbd:1,cdr:1,cd_min:{from_date},cd_max:{to_date}'\n",
    "            #\"tbs\": \"qdr:d\"\n",
    "            #\"tbs\": f\"sbd:1,lr:lang_en,cdr:1,cd_min:1/1/{from_year},cd_max:{to_year}\"\n",
    "        },\n",
    "        proxies=proxies,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    return resp\n",
    "\n",
    "def search(term, num_results=10, lang=\"en\", proxy=None, advanced=False, sleep_interval=0, timeout=5, from_date='1/1/2020', to_date='1/1/2020'):\n",
    "    \"\"\"Search the Google search engine\"\"\"\n",
    "\n",
    "    escaped_term = urllib.parse.quote_plus(term) # make 'site:xxx.xxx.xxx ' works.\n",
    "\n",
    "    # Proxy\n",
    "    proxies = None\n",
    "    if proxy:\n",
    "        if proxy.startswith(\"https\"):\n",
    "            proxies = {\"https\": proxy}\n",
    "        else:\n",
    "            proxies = {\"http\": proxy}\n",
    "\n",
    "    # Fetch\n",
    "    start = 0\n",
    "    res = []\n",
    "    while start < num_results:\n",
    "        # Send request\n",
    "        resp = _req(escaped_term, num_results - start,\n",
    "                    lang, start, proxies, timeout, from_date, to_date)\n",
    "        # Parse\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        result_block = soup.find_all(\"div\", attrs={\"class\": \"SoaBEf\"})\n",
    "\n",
    "        if len(result_block)==0:\n",
    "            break\n",
    "\n",
    "        for result in result_block:\n",
    "            # Find url, title, description\n",
    "            url = result.find(\"a\", href=True)\n",
    "            title = result.find(\"div\", attrs={\"class\": \"n0jPhd ynAwRc MBeuO nDgy9d\"})\n",
    "            date = result.find(\"div\", attrs={\"class\": \"OSrXXb rbYSKb LfVVr\"})\n",
    "            obj = {}\n",
    "            obj[\"title\"] = None\n",
    "            obj[\"date\"] = None\n",
    "            obj[\"url\"] = None\n",
    "            start += 1\n",
    "\n",
    "            if title:\n",
    "                obj[\"title\"] = title.text\n",
    "            if date:\n",
    "                obj[\"date\"] = from_date\n",
    "            if url:\n",
    "                obj[\"url\"] = url[\"href\"]\n",
    "                res.append(obj)\n",
    "                        \n",
    "        sleep(sleep_interval)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2fde9b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_news_urls(q, total, from_date, to_date, sleep_interval=0):\n",
    "    urls = []\n",
    "    urls = search(q, num_results=total, from_date=from_date, to_date=to_date, sleep_interval=sleep_interval)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b254fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_news_urls(q, total, from_date, to_date, temp_save=False):\n",
    "    dir_name = str(TODAY)\n",
    "    if dir_name not in os.listdir():\n",
    "        cwd = os.getcwd()\n",
    "        os.mkdir(f'{cwd}/{dir_name}')\n",
    "        \n",
    "    urls = get_google_news_urls(q, total, from_date, to_date, sleep_interval = 10)\n",
    "    # temp\n",
    "    if temp_save:\n",
    "        # replace date format to file savable characters\n",
    "        filename = f'{query}_{from_date.replace(\"/\", \"_\")}.json'\n",
    "        f = open(f'./{TODAY}/{filename}', 'w')\n",
    "        f.write(json.dumps(json_object, indent = 4))\n",
    "        f.close()\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "91f42a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_data(url):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        article = requests.get(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            timeout=request_timeout\n",
    "        )\n",
    "        status_code = article.status_code\n",
    "        if status_code >= 400:\n",
    "            f = open(f'./{TODAY}/ERROR_LOG.txt', 'a')\n",
    "            f.write(f'[{time.asctime(time.localtime())}] Code {status_code}: {url}\\n')\n",
    "            f.close()\n",
    "            return None\n",
    "        soup = bs(article.content, \"html.parser\")\n",
    "        article_body = soup.find(\"body\")\n",
    "        paragraphs = article_body.find_all(\"p\")\n",
    "        #sleep(1)\n",
    "        if paragraphs is not None:\n",
    "            for p in paragraphs:\n",
    "                text += re.sub(html_pattern, '', p.text).strip() + ' '  \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        msg = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ef8e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(q, json_object, date):\n",
    "    total_count = 0\n",
    "    for data in json_object:  \n",
    "        try:\n",
    "            data['text'] = get_news_data(data['url'])\n",
    "            if data is None:\n",
    "                continue\n",
    "            if q not in os.listdir(f'./{TODAY}'):\n",
    "                cwd = os.getcwd()\n",
    "                os.mkdir(f'{cwd}/{TODAY}/{q}')\n",
    "            total_count += 1\n",
    "            file_error_symbols = []\n",
    "            filename = f'{total_count}_{date.replace(\"/\", \"_\")}'\n",
    "            f = open(f'./{TODAY}/{q}//{filename}.json', \"w\")\n",
    "            f.write(json.dumps(data, indent = 4))\n",
    "            f.close()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "    print(f'Saved {total_count} news data in {date}')\n",
    "    #print(os.system(\"npx prettier -w ./dataset/*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c56ef85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_from_file(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2fdadc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_from_file(filename):\n",
    "    keywords = []\n",
    "    f = open(filename, \"r\")\n",
    "    for keyword in f:\n",
    "        keywords.append(keyword)\n",
    "    f.close()\n",
    "    return list(map(lambda keyword: keyword.strip(), keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "349e49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_save_dataset(q, urls):\n",
    "    start = time.time()\n",
    "    for url in urls:\n",
    "        print(urls)\n",
    "        save_dataset(q, urls[year], year)\n",
    "    #Parallel(n_jobs = -1)(delayed(save_dataset)(q, urls[year], year) for year in urls)\n",
    "    end = time.time()\n",
    "    print('{:.2f} seconds used'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "deaf2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run all functions to save news data\n",
    "def scrap_news_data(total, from_date, to_date, keywords_filename=\"keywords.txt\"):\n",
    "    #count used time\n",
    "    start = time.time()\n",
    "    #step 1 - load keywords file\n",
    "    keywords = get_keywords_from_file(keywords_filename)\n",
    "    for query in keywords:\n",
    "        print(f'Scrapping {query} ...')\n",
    "        #step 2 - save urls to json file\n",
    "        save_news_urls(query, total, from_date, to_date)\n",
    "        #step 3 - get urls object from previous saved json file\n",
    "        urls = get_urls_from_file(f'./{TODAY}/{query}.json')\n",
    "        #step 4 - web scrapping news data from specific year of urls & save into directory seperately\n",
    "        save_dataset(query, urls, from_date)\n",
    "        #parallel_save_dataset(query, urls, date)\n",
    "        #save_dataset(query, urls)\n",
    "        \n",
    "    end = time.time()\n",
    "    print('[DONE] {:.2f} seconds used'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "84a37454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv(q, json_obj, index):\n",
    "    text = None\n",
    "    title = None\n",
    "    date = None\n",
    "    url = json_obj['url']\n",
    "    if 'date' in json_obj.keys():\n",
    "        date = json_obj['date']\n",
    "    if json_obj['text'] is not None:\n",
    "        text = re.sub(r'[,.\\'\"|]', '', json_obj['text'])\n",
    "    if 'title' in json_obj.keys() and json_obj['title'] != None:\n",
    "        title = re.sub(r'[,.\\'\"|]', '', json_obj['title'])\n",
    "    if text != '' and text != None:\n",
    "        text = ' '.join(text.encode('utf-8').decode().split())\n",
    "    if title != '' and title != None:\n",
    "        title = ' '.join(title.split()).strip() \n",
    "    f = open(f'./{TODAY}/{q}.csv', 'a', encoding='utf-8')\n",
    "    f.write(f'{index},\\\"{title}\\\",\\\"{date}\\\",\\\"{url}\\\",\\\"{text}\\\"\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "49f6fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date_time_format(date):\n",
    "    date = date.split('/')\n",
    "    date = list(map(lambda x: int(x), date))\n",
    "    return datetime.datetime(date[2], date[1], date[0])\n",
    "\n",
    "def to_google_param_format(date):\n",
    "    return f'{date.month}/{date.day}/{date.year}'\n",
    "\n",
    "def save_json_object(filename, json_object):\n",
    "    f = open(f'./{TODAY}/{filename}', 'w')\n",
    "    f.write(json.dumps(json_object, indent = 4))\n",
    "    f.close()\n",
    "    \n",
    "def get_date_range_list(from_date, to_date):\n",
    "    from_date_time = to_date_time_format(from_date)\n",
    "    to_date_time = to_date_time_format(to_date)\n",
    "    day_difference = (to_date_time - from_date_time).days\n",
    "    datetime_list = [from_date_time + datetime.timedelta(days=x) for x in range(day_difference + 1)]\n",
    "    date_range_list = list(map(lambda x: to_google_param_format(x), datetime_list))\n",
    "    return date_range_list\n",
    "    \n",
    "def parallel_save_dataset(query, from_date, to_date):\n",
    "    # count used time\n",
    "    start = time.time()\n",
    "    # JSON object\n",
    "    json_object = dict()\n",
    "    # get from date & to date difference days\n",
    "    date_range_list = get_date_range_list(from_date, to_date)\n",
    "    # iterate through date range list\n",
    "    for date in date_range_list:\n",
    "        urls = save_news_urls(query, total_news, date, date)\n",
    "        json_object[date] = urls\n",
    "        \n",
    "    # replace date format to file savable characters\n",
    "    filename = f'{query}.json'\n",
    "    # temporary save of the scrapped urls\n",
    "    save_json_object(filename, json_object)\n",
    "    for date in json_object:\n",
    "        save_dataset(query, json_object[date], date)\n",
    "    print(f'{filename}: [DONE]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0761ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_json_to_csv():\n",
    "    cols = \"title,date,url,text\\n\"\n",
    "    keywords = get_keywords_from_file('keywords.txt')\n",
    "    for keyword in keywords:\n",
    "        f = open(f'./{TODAY}/{keyword}.csv', 'w')\n",
    "        f.write(cols)\n",
    "        f.close()\n",
    "        for index, filename in enumerate(os.listdir(f'./{TODAY}/{keyword}')):\n",
    "            if '.json' not in filename:\n",
    "                continue\n",
    "            json_file = open(f'./{TODAY}/{keyword}/{filename}', 'r')\n",
    "            json_obj = json.load(json_file)\n",
    "            json_file.close()\n",
    "            json_to_csv(keyword, json_obj, index)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "95b07911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_text(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    special_characters = \"!@#$%^&*()-+?_=,<>\\\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_words = word_tokenize(text.lower())\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_words]\n",
    "    preprocessed_words = [word for word in lemmatized_words if word not in stop_words]\n",
    "    preprocessed_words = [word for word in lemmatized_words if word not in special_characters]\n",
    "    preprocessed_text = \" \".join(preprocessed_words)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f373e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score(text):\n",
    "    return sentiment_pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "01fc43d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 news data in 1/1/2024\n",
      "Saved 10 news data in 1/2/2024\n",
      "Saved 10 news data in 1/3/2024\n",
      "Saved 10 news data in 1/4/2024\n",
      "Saved 10 news data in 1/5/2024\n",
      "Saved 10 news data in 1/6/2024\n",
      "Saved 8 news data in 1/7/2024\n",
      "Saved 10 news data in 1/8/2024\n",
      "Saved 10 news data in 1/9/2024\n",
      "Saved 10 news data in 1/10/2024\n",
      "Saved 10 news data in 1/11/2024\n",
      "Saved 10 news data in 1/12/2024\n",
      "Saved 10 news data in 1/13/2024\n",
      "Saved 10 news data in 1/14/2024\n",
      "Saved 10 news data in 1/15/2024\n",
      "Saved 10 news data in 1/16/2024\n",
      "Saved 10 news data in 1/17/2024\n",
      "Saved 10 news data in 1/18/2024\n",
      "Saved 10 news data in 1/19/2024\n",
      "Saved 3 news data in 1/20/2024\n",
      "Saved 10 news data in 1/21/2024\n",
      "Saved 10 news data in 1/22/2024\n",
      "Saved 10 news data in 1/23/2024\n",
      "Saved 10 news data in 1/24/2024\n",
      "Saved 10 news data in 1/25/2024\n",
      "Saved 10 news data in 1/26/2024\n",
      "Saved 10 news data in 1/27/2024\n",
      "Saved 10 news data in 1/28/2024\n",
      "Saved 10 news data in 1/29/2024\n",
      "Saved 10 news data in 1/30/2024\n",
      "Saved 10 news data in 1/31/2024\n",
      "Saved 10 news data in 2/1/2024\n",
      "Saved 10 news data in 2/2/2024\n",
      "Saved 10 news data in 2/3/2024\n",
      "Saved 10 news data in 2/4/2024\n",
      "Saved 10 news data in 2/5/2024\n",
      "Saved 10 news data in 2/6/2024\n",
      "Saved 10 news data in 2/7/2024\n",
      "Saved 10 news data in 2/8/2024\n",
      "Saved 10 news data in 2/9/2024\n",
      "Saved 10 news data in 2/10/2024\n",
      "Saved 10 news data in 2/11/2024\n",
      "Saved 10 news data in 2/12/2024\n",
      "Saved 10 news data in 2/13/2024\n",
      "Saved 10 news data in 2/14/2024\n",
      "Saved 10 news data in 2/15/2024\n",
      "Saved 10 news data in 2/16/2024\n",
      "Saved 10 news data in 2/17/2024\n",
      "Saved 10 news data in 2/18/2024\n",
      "Saved 10 news data in 2/19/2024\n",
      "Saved 10 news data in 2/20/2024\n",
      "Saved 10 news data in 2/21/2024\n",
      "Saved 10 news data in 2/22/2024\n",
      "Saved 10 news data in 2/23/2024\n",
      "Saved 10 news data in 2/24/2024\n",
      "Saved 10 news data in 2/25/2024\n",
      "Saved 10 news data in 2/26/2024\n",
      "Saved 10 news data in 2/27/2024\n",
      "Saved 10 news data in 2/28/2024\n",
      "Saved 10 news data in 2/29/2024\n",
      "Saved 10 news data in 3/1/2024\n",
      "AAPL.json: [DONE]\n"
     ]
    }
   ],
   "source": [
    "parallel_save_dataset('AAPL', from_date, to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e6b7d35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "all_json_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "28cfe03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Invest with Confidence: Intrinsic Value Unveil...</td>\n",
       "      <td>1/1/2024</td>\n",
       "      <td>https://finance.yahoo.com/news/invest-confiden...</td>\n",
       "      <td>In this article we will take a look into Apple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Apple: Accept The Gift Move On (NASDAQ:AAPL)</td>\n",
       "      <td>1/1/2024</td>\n",
       "      <td>https://seekingalpha.com/article/4660598-apple...</td>\n",
       "      <td>Petra Schüller/iStock via Getty Images Apple (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Which Magnificent Seven Stocks Are Screaming B...</td>\n",
       "      <td>1/1/2024</td>\n",
       "      <td>https://finance.yahoo.com/news/magnificent-sev...</td>\n",
       "      <td>The Magnificent Seven stocks dominated the mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>TSMC: Major Apple Risk Persists But Stock Stil...</td>\n",
       "      <td>1/1/2024</td>\n",
       "      <td>https://seekingalpha.com/article/4660614-tsmc-...</td>\n",
       "      <td>Annabelle Chih/Getty Images News Annabelle Chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PC market decline decelerating Apple holding firm</td>\n",
       "      <td>1/10/2024</td>\n",
       "      <td>https://appleinsider.com/articles/24/01/10/jp-...</td>\n",
       "      <td>Copyright © 2024 Quiller Media Inc Contact Us ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>Apple car a gigantic money pit: Lutz</td>\n",
       "      <td>3/1/2024</td>\n",
       "      <td>https://www.youtube.com/watch?v=hoet0wzGjJ8</td>\n",
       "      <td>Your browser isnâ€™t supported anymore Update ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>The Apple Car crash The Vergecast</td>\n",
       "      <td>3/1/2024</td>\n",
       "      <td>https://www.youtube.com/watch?v=Eis2yyCOVHc</td>\n",
       "      <td>Your browser isnâ€™t supported anymore Update ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>How Apple is Juicing Payments – Digital Transa...</td>\n",
       "      <td>3/1/2024</td>\n",
       "      <td>https://www.digitaltransactions.net/magazine_a...</td>\n",
       "      <td>John Stewart March 1 2024 The iPhone maker is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>Apple bull vs bear debate: Whats next for the ...</td>\n",
       "      <td>3/1/2024</td>\n",
       "      <td>https://www.youtube.com/watch?v=dAMRdOlfJPo</td>\n",
       "      <td>Your browser isnâ€™t supported anymore Update ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>microLED Apple Watch Ultra canceled: Heres wha...</td>\n",
       "      <td>3/1/2024</td>\n",
       "      <td>https://bgr.com/tech/the-apple-watch-ultra-mic...</td>\n",
       "      <td>Sign up for our daily newsletter If you buy th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title       date  \\\n",
       "252  Invest with Confidence: Intrinsic Value Unveil...   1/1/2024   \n",
       "191       Apple: Accept The Gift Move On (NASDAQ:AAPL)   1/1/2024   \n",
       "130  Which Magnificent Seven Stocks Are Screaming B...   1/1/2024   \n",
       "69   TSMC: Major Apple Risk Persists But Stock Stil...   1/1/2024   \n",
       "1    PC market decline decelerating Apple holding firm  1/10/2024   \n",
       "..                                                 ...        ...   \n",
       "361               Apple car a gigantic money pit: Lutz   3/1/2024   \n",
       "538                  The Apple Car crash The Vergecast   3/1/2024   \n",
       "58   How Apple is Juicing Payments – Digital Transa...   3/1/2024   \n",
       "479  Apple bull vs bear debate: Whats next for the ...   3/1/2024   \n",
       "596  microLED Apple Watch Ultra canceled: Heres wha...   3/1/2024   \n",
       "\n",
       "                                                   url  \\\n",
       "252  https://finance.yahoo.com/news/invest-confiden...   \n",
       "191  https://seekingalpha.com/article/4660598-apple...   \n",
       "130  https://finance.yahoo.com/news/magnificent-sev...   \n",
       "69   https://seekingalpha.com/article/4660614-tsmc-...   \n",
       "1    https://appleinsider.com/articles/24/01/10/jp-...   \n",
       "..                                                 ...   \n",
       "361        https://www.youtube.com/watch?v=hoet0wzGjJ8   \n",
       "538        https://www.youtube.com/watch?v=Eis2yyCOVHc   \n",
       "58   https://www.digitaltransactions.net/magazine_a...   \n",
       "479        https://www.youtube.com/watch?v=dAMRdOlfJPo   \n",
       "596  https://bgr.com/tech/the-apple-watch-ultra-mic...   \n",
       "\n",
       "                                                  text  \n",
       "252  In this article we will take a look into Apple...  \n",
       "191  Petra Schüller/iStock via Getty Images Apple (...  \n",
       "130  The Magnificent Seven stocks dominated the mar...  \n",
       "69   Annabelle Chih/Getty Images News Annabelle Chi...  \n",
       "1    Copyright © 2024 Quiller Media Inc Contact Us ...  \n",
       "..                                                 ...  \n",
       "361  Your browser isnâ€™t supported anymore Update ...  \n",
       "538  Your browser isnâ€™t supported anymore Update ...  \n",
       "58   John Stewart March 1 2024 The iPhone maker is ...  \n",
       "479  Your browser isnâ€™t supported anymore Update ...  \n",
       "596  Sign up for our daily newsletter If you buy th...  \n",
       "\n",
       "[495 rows x 4 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv(f\"./{TODAY}/AAPL.csv\")\n",
    "news = news.mask(news.eq('None')).dropna(subset=['text']).sort_values(by=['date'])\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0441cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(news)):\n",
    "    data = news.iloc[i]\n",
    "    title = data['title']\n",
    "    text = data['text']\n",
    "    preprocessed_text = get_preprocessed_text(text)\n",
    "    sentiment_result = sentiment_pipe(preprocessed_text)[0]\n",
    "    print(f\"{title}\\n[{sentiment_result['label']}] {sentiment_result['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caef588a",
   "metadata": {},
   "source": [
    "# Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\"https://finance.yahoo.com/quote/NVDA/news?p=NVDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3103f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "\n",
    "nvda = yf.Ticker(\"NVDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = nvda.news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f38fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in news:\n",
    "    title = i['title']\n",
    "    date = datetime.fromtimestamp(i['providerPublishTime'])\n",
    "    print(date, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca030fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = get_news_data(url=news[0]['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "special_characters = \"!@#$%^&*()-+?_=,<>\\\"\"\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fc7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = word_tokenize(article.lower())\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_words = [word for word in lemmatized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90626178",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_words = [word for word in lemmatized_words if word not in special_characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d00f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = \" \".join(preprocessed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105835c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be23844",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe(out + out + out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc71aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8857f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68def5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
