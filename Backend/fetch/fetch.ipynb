{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ef6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import string\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246b796",
   "metadata": {},
   "source": [
    "# Google Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dcd7fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_useragent():\n",
    "    return random.choice(_useragent_list)\n",
    "\n",
    "_useragent_list = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36 Edg/111.0.1661.62',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0'\n",
    "]\n",
    "\n",
    "request_timeout = 10\n",
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:49.0) Gecko/20100101 Firefox/49.0',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Cache-Control': 'private, max-age=0',\n",
    "}\n",
    "html_pattern = \"<(?:\\\"[^\\\"]*\\\"['\\\"]*|'[^']*'['\\\"]*|[^'\\\">])+>\"\n",
    "TODAY = datetime.date.today()\n",
    "\n",
    "#search variables\n",
    "from_year = 2023\n",
    "to_year = 2024\n",
    "total_news_per_year = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ceba417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"googlesearch is a Python library for searching Google, easily.\"\"\"\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import urllib\n",
    "import random\n",
    "\n",
    "def get_headers(): \n",
    "    headers = {\n",
    "        'User-Agent': get_useragent(),\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Cache-Control': 'private, max-age=0',\n",
    "    }\n",
    "\n",
    "    return headers\n",
    "\n",
    "def _req(term, results, lang, start, proxies, timeout, from_year, to_year):\n",
    "    resp = get(\n",
    "        url=\"https://www.google.com/search\",\n",
    "        headers={\n",
    "            \"User-Agent\": get_useragent()\n",
    "        },\n",
    "        params={\n",
    "            \"q\": term,\n",
    "            \"num\": results,  # Prevents multiple requests\n",
    "            \"hl\": lang,\n",
    "            \"start\": start,\n",
    "            \"lr\": \"lang_en\",\n",
    "            \"tbm\": 'nws',\n",
    "            \"tbs\": \"qdr:d\"\n",
    "            #\"tbs\": f\"sbd:1,lr:lang_en,cdr:1,cd_min:1/1/{from_year},cd_max:{to_year}\"\n",
    "        },\n",
    "        proxies=proxies,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    return resp\n",
    "\n",
    "def search(term, num_results=10, lang=\"en\", proxy=None, advanced=False, sleep_interval=0, timeout=5, from_year=2020, to_year=2020):\n",
    "    \"\"\"Search the Google search engine\"\"\"\n",
    "\n",
    "    escaped_term = urllib.parse.quote_plus(term) # make 'site:xxx.xxx.xxx ' works.\n",
    "\n",
    "    # Proxy\n",
    "    proxies = None\n",
    "    if proxy:\n",
    "        if proxy.startswith(\"https\"):\n",
    "            proxies = {\"https\": proxy}\n",
    "        else:\n",
    "            proxies = {\"http\": proxy}\n",
    "\n",
    "    # Fetch\n",
    "    start = 0\n",
    "    res = []\n",
    "    while start < num_results:\n",
    "        # Send request\n",
    "        resp = _req(escaped_term, num_results - start,\n",
    "                    lang, start, proxies, timeout, from_year, to_year)\n",
    "        # Parse\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        result_block = soup.find_all(\"div\", attrs={\"class\": \"SoaBEf\"})\n",
    "\n",
    "        if len(result_block)==0:\n",
    "            break\n",
    "\n",
    "        for result in result_block:\n",
    "            # Find url, title, description\n",
    "            url = result.find(\"a\", href=True)\n",
    "            title = result.find(\"div\", attrs={\"class\": \"n0jPhd ynAwRc MBeuO nDgy9d\"})\n",
    "            date = result.find(\"div\", attrs={\"class\": \"OSrXXb rbYSKb LfVVr\"})\n",
    "            obj = {}\n",
    "            obj[\"title\"] = None\n",
    "            obj[\"date\"] = None\n",
    "            obj[\"url\"] = None\n",
    "            start += 1\n",
    "\n",
    "            if title:\n",
    "                obj[\"title\"] = title.text\n",
    "            if date:\n",
    "                obj[\"date\"] = date.text\n",
    "            if url:\n",
    "                obj[\"url\"] = url[\"href\"]\n",
    "                res.append(obj)\n",
    "                        \n",
    "        sleep(sleep_interval)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fde9b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_news_urls(q, total, from_year, to_year, sleep_interval=90):\n",
    "    urls = []\n",
    "    urls = search(q, num_results=total, from_year=from_year, to_year=to_year, sleep_interval=sleep_interval)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b254fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_news_urls(q, total, from_year, to_year):\n",
    "    dir_name = str(TODAY)\n",
    "    filename = f'{q}.json'\n",
    "    \n",
    "    if dir_name not in os.listdir():\n",
    "        cwd = os.getcwd()\n",
    "        os.mkdir(f'{cwd}/{dir_name}')\n",
    "        \n",
    "    if filename in os.listdir(f'./{dir_name}'):\n",
    "        print(f'{q}.json exists.')\n",
    "        return None\n",
    "    \n",
    "    urls = {}\n",
    "    for year in range(from_year, to_year + 1):\n",
    "        urls[str(year)] = get_google_news_urls(q, total, year, year)\n",
    "        print(f'Year {year} News urls collection - Done')\n",
    "        #delay request to prevent 429 Too many Request\n",
    "    print()\n",
    "    \n",
    "    f = open(f'./{TODAY}/{filename}', 'w')\n",
    "    f.write(json.dumps(urls, indent = 4))\n",
    "    f.close()\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91f42a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_data(url):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        article = requests.get(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            timeout=request_timeout\n",
    "        )\n",
    "        status_code = article.status_code\n",
    "        if status_code >= 400:\n",
    "            f = open(f'./{TODAY}/ERROR_LOG.txt', 'a')\n",
    "            f.write(f'[{time.asctime(time.localtime())}] Code {status_code}: {url}\\n')\n",
    "            f.close()\n",
    "            return None\n",
    "        soup = bs(article.content, \"html.parser\")\n",
    "        article_body = soup.find(\"div\", {\"class\": \"caas-body\"})\n",
    "        paragraphs = article_body.find_all(\"p\")\n",
    "        sleep(1)\n",
    "        if paragraphs is not None:\n",
    "            for p in paragraphs:\n",
    "                text += re.sub(html_pattern, '', p.text).strip() + ' '  \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        msg = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ef8e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(q, urls, year):\n",
    "    total_count = 0\n",
    "    for url in urls:\n",
    "        try:\n",
    "            data = url\n",
    "            data['text'] = get_news_data(data['url'])\n",
    "            if data is None:\n",
    "                continue\n",
    "            if q not in os.listdir(f'./{TODAY}'):\n",
    "                cwd = os.getcwd()\n",
    "                os.mkdir(f'{cwd}/{TODAY}/{q}')\n",
    "            total_count += 1\n",
    "            file_error_symbols = []\n",
    "            filename = f'{year}-{total_count}'\n",
    "            f = open(f'./{TODAY}/{q}/{filename}.json', \"w\")\n",
    "            f.write(json.dumps(data, indent = 4))\n",
    "            f.close()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "    print(f'Saved {total_count} news data in year {year}')\n",
    "    #print(os.system(\"npx prettier -w ./dataset/*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c56ef85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_from_file(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fdadc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_from_file(filename):\n",
    "    keywords = []\n",
    "    f = open(filename, \"r\")\n",
    "    for keyword in f:\n",
    "        keywords.append(keyword)\n",
    "    f.close()\n",
    "    return list(map(lambda keyword: keyword.strip(), keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "349e49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_save_dataset(q, urls):\n",
    "    start = time.time()\n",
    "    Parallel(n_jobs = -1)(delayed(save_dataset)(q, urls[year], year) for year in urls)\n",
    "    end = time.time()\n",
    "    print('{:.2f} seconds used'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "deaf2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run all functions to save news data\n",
    "def scrap_news_data(total, from_year, to_year, keywords_filename=\"keywords.txt\"):\n",
    "    #count used time\n",
    "    start = time.time()\n",
    "    #step 1 - load keywords file\n",
    "    keywords = get_keywords_from_file(keywords_filename)\n",
    "    for query in keywords:\n",
    "        print(f'Scrapping {query} ...')\n",
    "        #step 2 - save urls to json file\n",
    "        save_news_urls(query, total, from_year, to_year)\n",
    "        #step 3 - get urls object from previous saved json file\n",
    "        urls = get_urls_from_file(f'./{TODAY}/{query}.json')\n",
    "        #step 4 - web scrapping news data from specific year of urls & save into directory seperately\n",
    "        parallel_save_dataset(query, urls)\n",
    "        \n",
    "    end = time.time()\n",
    "    print('[DONE] {:.2f} seconds used'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84a37454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv(q, json_obj, index):\n",
    "    text = None\n",
    "    title = None\n",
    "    date = None\n",
    "    url = json_obj['url']\n",
    "    if 'date' in json_obj.keys():\n",
    "        date = json_obj['date']\n",
    "    if json_obj['text'] is not None:\n",
    "        text = re.sub(r'[,.\\'\"|]', '', json_obj['text'])\n",
    "    if 'title' in json_obj.keys() and json_obj['title'] != None:\n",
    "        title = re.sub(r'[,.\\'\"|]', '', json_obj['title'])\n",
    "    if text != '' and text != None:\n",
    "        text = ' '.join(text.encode('utf-8').decode().split())\n",
    "    if title != '' and title != None:\n",
    "        title = ' '.join(title.split()).strip() \n",
    "    f = open(f'./{TODAY}/{q}.csv', 'a', encoding='utf-8')\n",
    "    f.write(f'{index},\\\"{title}\\\",\\\"{date}\\\",\\\"{url}\\\",\\\"{text}\\\"\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0761ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_json_to_csv():\n",
    "    cols = \"title,date,url,text\\n\"\n",
    "    keywords = get_keywords_from_file('keywords.txt')\n",
    "    for keyword in keywords:\n",
    "        f = open(f'./{TODAY}/{keyword}.csv', 'w')\n",
    "        f.write(cols)\n",
    "        f.close()\n",
    "        for index, filename in enumerate(os.listdir(f'./{TODAY}/{keyword}')):\n",
    "            json_file = open(f'./{TODAY}/{keyword}/{filename}', 'r')\n",
    "            json_obj = json.load(json_file)\n",
    "            json_file.close()\n",
    "            json_to_csv(keyword, json_obj, index)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb9ddf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv():\n",
    "    keywords = get_keywords_from_file('keywords.txt')\n",
    "    dataframes = []\n",
    "    for keyword in keywords:\n",
    "        df = pd.read_csv(f'./{TODAY}/{keyword}.csv', on_bad_lines='skip')\n",
    "        dataframes.append(df)\n",
    "    dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "    # drop duplicate url\n",
    "    dataframe = dataframe.drop_duplicates(subset=['url'])\n",
    "    dataframe.to_csv(f'./{TODAY}/dataset.csv')\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0568eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping AAPL ...\n",
      "Year 2023 News urls collection - Done\n",
      "Year 2024 News urls collection - Done\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"c:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"c:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\joblib\\parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n  File \"c:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\joblib\\parallel.py\", line 589, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\endyj\\AppData\\Local\\Temp\\ipykernel_2064\\1701118362.py\", line 6, in save_dataset\n  File \"C:\\Users\\endyj\\AppData\\Local\\Temp\\ipykernel_2064\\1341117370.py\", line 17, in get_news_data\nAttributeError: 'NoneType' object has no attribute 'find_all'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mscrap_news_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_news_per_year\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_year\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_year\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 14\u001b[0m, in \u001b[0;36mscrap_news_data\u001b[1;34m(total, from_year, to_year, keywords_filename)\u001b[0m\n\u001b[0;32m     12\u001b[0m     urls \u001b[38;5;241m=\u001b[39m get_urls_from_file(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTODAY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#step 4 - web scrapping news data from specific year of urls & save into directory seperately\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mparallel_save_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[DONE] \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m seconds used\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(end \u001b[38;5;241m-\u001b[39m start))\n",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m, in \u001b[0;36mparallel_save_dataset\u001b[1;34m(q, urls)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_save_dataset\u001b[39m(q, urls):\n\u001b[0;32m      2\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murls\u001b[49m\u001b[43m[\u001b[49m\u001b[43myear\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m seconds used\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(end \u001b[38;5;241m-\u001b[39m start))\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1693\u001b[0m \n\u001b[0;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1699\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1700\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\joblib\\parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1734\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\joblib\\parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    730\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\joblib\\parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "scrap_news_data(total_news_per_year, from_year, to_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caef588a",
   "metadata": {},
   "source": [
    "# Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5595d84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [404]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"https://finance.yahoo.com/quote/NVDA/news?p=NVDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3103f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "\n",
    "nvda = yf.Ticker(\"NVDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9c2737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = nvda.news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "370f38fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-09 19:00:00 Best AI Stock 2024: Nvidia Stock vs. IBM Stock\n",
      "2024-01-09 18:53:00 These Stocks Are Moving the Most Today: Boeing, Juniper Networks, HPE, Nvidia, Unity Software, Match, and More\n",
      "2024-01-09 18:30:00 Nvidia Stock Is Over $500 -- Here Is What Investors Should Know About Recent Updates\n",
      "2024-01-09 18:05:00 Better Artificial Intelligence (AI) Stock: Nvidia vs. Alphabet\n",
      "2024-01-09 16:24:00 Heard on the Street Recap: Maxing Out\n",
      "2024-01-09 10:40:44 Dow Jones Futures Fall: Magnificent Seven Stocks Rally; Nvidia Breaks Out Past Buy Point\n",
      "2024-01-09 09:55:50 CES 2024: How to watch live as Sony, Samsung and more reveal hardware, AI updates\n",
      "2024-01-09 07:57:00 Markets Party Like It's 2023!\n"
     ]
    }
   ],
   "source": [
    "for i in news:\n",
    "    title = i['title']\n",
    "    date = datetime.fromtimestamp(i['providerPublishTime'])\n",
    "    print(date, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aca030fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = get_news_data(url=news[6]['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "318b29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "special_characters = \"!@#$%^&*()-+?_=,<>\\\"\"\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "66fc7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = word_tokenize(article.lower())\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5d5f8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_words = [word for word in lemmatized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "90626178",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_words = [word for word in lemmatized_words if word not in special_characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84d00f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = \" \".join(preprocessed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "105835c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
    "\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5be23844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2285 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"embeddings\" \"                 f\"(type TFRobertaEmbeddings).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,512] = 514 is not in [0, 514) [Op:ResourceGather]\n\nCall arguments received by layer \"embeddings\" \"                 f\"(type TFRobertaEmbeddings):\n  • input_ids=tf.Tensor(shape=(1, 2285), dtype=int32)\n  • position_ids=None\n  • token_type_ids=tf.Tensor(shape=(1, 2285), dtype=int32)\n  • inputs_embeds=None\n  • past_key_values_length=0\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msentiment_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\transformers\\pipelines\\base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1134\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         )\n\u001b[0;32m   1138\u001b[0m     )\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\transformers\\pipelines\\base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1146\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1147\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\transformers\\pipelines\\base.py:1041\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1040\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1041\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1043\u001b[0m     inference_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_inference_context()\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    186\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\transformers\\modeling_tf_utils.py:426\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    423\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    425\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py:1277\u001b[0m, in \u001b[0;36mTFRobertaForSequenceClassification.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(ROBERTA_INPUTS_DOCSTRING\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size, sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1270\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFSequenceClassifierOutput, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;124;03m    labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;124;03m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;124;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;124;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1277\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1289\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1290\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output, training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\transformers\\modeling_tf_utils.py:426\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    423\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    425\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py:654\u001b[0m, in \u001b[0;36mTFRobertaMainLayer.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    652\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfill(dims\u001b[38;5;241m=\u001b[39minput_shape, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 654\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;66;03m# We create a 3D attention mask from a 2D tensor mask.\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;66;03m# Sizes are [batch_size, 1, 1, to_seq_length]\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;66;03m# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m# this attention mask is more simple than the triangular masking of causal attention\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;66;03m# used in OpenAI GPT, we just need to prepare the broadcast dimension here.\u001b[39;00m\n\u001b[0;32m    668\u001b[0m attention_mask_shape \u001b[38;5;241m=\u001b[39m shape_list(attention_mask)\n",
      "File \u001b[1;32mc:\\Users\\endyj\\anaconda3\\envs\\fyp\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py:167\u001b[0m, in \u001b[0;36mTFRobertaEmbeddings.call\u001b[1;34m(self, input_ids, position_ids, token_type_ids, inputs_embeds, past_key_values_length, training)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(\n\u001b[0;32m    164\u001b[0m             tf\u001b[38;5;241m.\u001b[39mrange(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, limit\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    165\u001b[0m         )\n\u001b[1;32m--> 167\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m token_type_embeds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings, indices\u001b[38;5;241m=\u001b[39mtoken_type_ids)\n\u001b[0;32m    169\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds \u001b[38;5;241m+\u001b[39m token_type_embeds\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"embeddings\" \"                 f\"(type TFRobertaEmbeddings).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,512] = 514 is not in [0, 514) [Op:ResourceGather]\n\nCall arguments received by layer \"embeddings\" \"                 f\"(type TFRobertaEmbeddings):\n  • input_ids=tf.Tensor(shape=(1, 2285), dtype=int32)\n  • position_ids=None\n  • token_type_ids=tf.Tensor(shape=(1, 2285), dtype=int32)\n  • inputs_embeds=None\n  • past_key_values_length=0\n  • training=False"
     ]
    }
   ],
   "source": [
    "sentiment_pipe(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1fc71aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1426"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d8857f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ce',\n",
       " '2024',\n",
       " 'is',\n",
       " 'finally',\n",
       " 'upon',\n",
       " 'u',\n",
       " 'taking',\n",
       " 'over',\n",
       " 'la',\n",
       " 'vega',\n",
       " 'with',\n",
       " 'throng',\n",
       " 'of',\n",
       " 'crowd',\n",
       " 'booth',\n",
       " 'full',\n",
       " 'of',\n",
       " 'product',\n",
       " 'and',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'company',\n",
       " 'making',\n",
       " 'claim',\n",
       " 'about',\n",
       " 'how',\n",
       " 'ai',\n",
       " 'is',\n",
       " 'improving',\n",
       " 'their',\n",
       " 'offering',\n",
       " '.',\n",
       " 'a',\n",
       " 'noted',\n",
       " 'in',\n",
       " 'our',\n",
       " 'ce',\n",
       " 'preview',\n",
       " 'though',\n",
       " 'the',\n",
       " 'conference',\n",
       " 'ha',\n",
       " 'had',\n",
       " 'it',\n",
       " 'ups',\n",
       " 'and',\n",
       " 'down',\n",
       " 'of',\n",
       " 'late',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'increasingly',\n",
       " 'become',\n",
       " 'an',\n",
       " 'opportunity',\n",
       " 'for',\n",
       " 'startup',\n",
       " 'to',\n",
       " 'capture',\n",
       " 'attention',\n",
       " 'while',\n",
       " 'all',\n",
       " 'eye',\n",
       " 'are',\n",
       " 'drawn',\n",
       " 'to',\n",
       " 'the',\n",
       " 'bigger',\n",
       " 'budget',\n",
       " 'announcement',\n",
       " 'from',\n",
       " 'the',\n",
       " 'like',\n",
       " 'of',\n",
       " 'samsung',\n",
       " 'sony',\n",
       " 'and',\n",
       " 'nvidia',\n",
       " '.',\n",
       " 'techcrunch',\n",
       " 'is',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ground',\n",
       " 'at',\n",
       " 'ce',\n",
       " '2024',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'event',\n",
       " 'next',\n",
       " 'week',\n",
       " 'with',\n",
       " 'a',\n",
       " 'particular',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'those',\n",
       " 'startup',\n",
       " 'that',\n",
       " 'might',\n",
       " 'be',\n",
       " 'headlining',\n",
       " 'a',\n",
       " 'big',\n",
       " 'livestream',\n",
       " 'of',\n",
       " 'their',\n",
       " 'own',\n",
       " 'in',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'year',\n",
       " '.',\n",
       " 'you',\n",
       " 'can',\n",
       " 'follow',\n",
       " 'along',\n",
       " 'with',\n",
       " 'our',\n",
       " 'team',\n",
       " '’',\n",
       " 's',\n",
       " 'ce',\n",
       " 'coverage',\n",
       " 'across',\n",
       " 'the',\n",
       " 'site',\n",
       " 'and',\n",
       " 'social',\n",
       " 'handle',\n",
       " 'here',\n",
       " 'but',\n",
       " 'let',\n",
       " '’',\n",
       " 's',\n",
       " 'cut',\n",
       " 'to',\n",
       " 'the',\n",
       " 'chase',\n",
       " 'since',\n",
       " 'we',\n",
       " 'all',\n",
       " 'know',\n",
       " 'those',\n",
       " 'big-name',\n",
       " 'event',\n",
       " 'still',\n",
       " 'matter',\n",
       " '.',\n",
       " 'consumer',\n",
       " 'tech',\n",
       " 'and',\n",
       " 'transportation',\n",
       " 'aficionado',\n",
       " 'have',\n",
       " 'had',\n",
       " 'plenty',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'on',\n",
       " 'monday',\n",
       " 'starting',\n",
       " 'at',\n",
       " '7',\n",
       " 'a.m.',\n",
       " 'pt',\n",
       " '/',\n",
       " '11',\n",
       " 'a.m.',\n",
       " 'et',\n",
       " 'with',\n",
       " 'many',\n",
       " 'of',\n",
       " 'the',\n",
       " 'highest-profile',\n",
       " 'press',\n",
       " 'conference',\n",
       " 'being',\n",
       " 'livestreamed',\n",
       " 'to',\n",
       " 'the',\n",
       " 'public',\n",
       " 'a',\n",
       " 'ha',\n",
       " 'become',\n",
       " 'the',\n",
       " 'norm',\n",
       " '.',\n",
       " 'these',\n",
       " 'event',\n",
       " 'will',\n",
       " 'set',\n",
       " 'the',\n",
       " 'stage',\n",
       " 'for',\n",
       " 'the',\n",
       " 'public',\n",
       " 'ce',\n",
       " 'show',\n",
       " 'floor',\n",
       " 'opening',\n",
       " 'january',\n",
       " '9',\n",
       " 'and',\n",
       " 'running',\n",
       " 'through',\n",
       " 'january',\n",
       " '12.',\n",
       " 'a',\n",
       " 'you',\n",
       " '’',\n",
       " 'll',\n",
       " 'see',\n",
       " 'in',\n",
       " 'the',\n",
       " 'rundown',\n",
       " 'below',\n",
       " 'ai',\n",
       " 'will',\n",
       " 'be',\n",
       " 'the',\n",
       " 'big',\n",
       " 'through-line',\n",
       " 'running',\n",
       " 'across',\n",
       " 'almost',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'big',\n",
       " 'event',\n",
       " 'a',\n",
       " 'ce',\n",
       " '2024',\n",
       " 'mark',\n",
       " 'the',\n",
       " 'first',\n",
       " 'iteration',\n",
       " 'of',\n",
       " 'the',\n",
       " 'event',\n",
       " 'fully',\n",
       " 'in',\n",
       " 'the',\n",
       " 'new',\n",
       " 'ai-centric',\n",
       " 'era',\n",
       " '.',\n",
       " 'we',\n",
       " '’',\n",
       " 'll',\n",
       " 'keep',\n",
       " 'this',\n",
       " 'list',\n",
       " 'updated',\n",
       " 'along',\n",
       " 'with',\n",
       " 'the',\n",
       " 'biggest',\n",
       " 'reveals',\n",
       " 'but',\n",
       " 'for',\n",
       " 'now',\n",
       " 'these',\n",
       " 'are',\n",
       " 'the',\n",
       " 'big-ticket',\n",
       " 'company',\n",
       " 'looking',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'big',\n",
       " 'splash',\n",
       " 'before',\n",
       " 'the',\n",
       " 'convention',\n",
       " 'door',\n",
       " 'open',\n",
       " 'and',\n",
       " 'ce',\n",
       " '2024',\n",
       " 'begin',\n",
       " 'for',\n",
       " 'in-person',\n",
       " 'attendee',\n",
       " 'tuesday',\n",
       " '.',\n",
       " 'ce',\n",
       " 'is',\n",
       " 'all',\n",
       " 'about',\n",
       " 'ai',\n",
       " 'this',\n",
       " 'year',\n",
       " 'and',\n",
       " 'amd',\n",
       " 'is',\n",
       " 'no',\n",
       " 'exception',\n",
       " '.',\n",
       " 'the',\n",
       " 'company',\n",
       " 'unveiled',\n",
       " 'second-generation',\n",
       " 'ai',\n",
       " 'pc',\n",
       " 'built',\n",
       " 'around',\n",
       " 'the',\n",
       " 'ryzen',\n",
       " '8000g',\n",
       " 'series',\n",
       " 'neural',\n",
       " 'processing',\n",
       " 'unit',\n",
       " 'and',\n",
       " 'a',\n",
       " 'new',\n",
       " 'radeon',\n",
       " 'rx',\n",
       " '7600',\n",
       " 'xt',\n",
       " 'graphic',\n",
       " 'card',\n",
       " 'and',\n",
       " 'spoke',\n",
       " 'to',\n",
       " 'the',\n",
       " 'previously',\n",
       " 'announced',\n",
       " 'automotive',\n",
       " 'grade',\n",
       " 'processor',\n",
       " '.',\n",
       " '[',\n",
       " 'youtube',\n",
       " 'http',\n",
       " ':',\n",
       " '//www.youtube.com/watch',\n",
       " 'v=lltpld0whio',\n",
       " 'version=3',\n",
       " 'rel=1',\n",
       " 'showsearch=0',\n",
       " 'showinfo=1',\n",
       " 'iv_load_policy=1',\n",
       " 'fs=1',\n",
       " 'hl=en-us',\n",
       " 'autohide=2',\n",
       " 'wmode=transparent',\n",
       " 'w=640',\n",
       " 'h=360',\n",
       " ']',\n",
       " 'ce',\n",
       " 'is',\n",
       " 'wasting',\n",
       " 'no',\n",
       " 'time',\n",
       " 'in',\n",
       " 'getting',\n",
       " 'to',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'main',\n",
       " 'event',\n",
       " '.',\n",
       " 'nvidia',\n",
       " 'is',\n",
       " 'coming',\n",
       " 'into',\n",
       " 'the',\n",
       " 'event',\n",
       " 'riding',\n",
       " 'high',\n",
       " 'on',\n",
       " 'it',\n",
       " 'recent',\n",
       " 'ai-fueled',\n",
       " 'growth',\n",
       " '.',\n",
       " 'so',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'no',\n",
       " 'surprise',\n",
       " 'the',\n",
       " 'company',\n",
       " 'promise',\n",
       " 'a',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'ai',\n",
       " 'and',\n",
       " 'content',\n",
       " 'creation',\n",
       " 'during',\n",
       " 'their',\n",
       " 'kickoff',\n",
       " 'address',\n",
       " 'at',\n",
       " 'ce',\n",
       " '.',\n",
       " '[',\n",
       " 'youtube',\n",
       " 'http',\n",
       " ':',\n",
       " '//www.youtube.com/watch',\n",
       " 'v=-jbsg2unk2k',\n",
       " 'version=3',\n",
       " 'rel=1',\n",
       " 'showsearch=0',\n",
       " 'showinfo=1',\n",
       " 'iv_load_policy=1',\n",
       " 'fs=1',\n",
       " 'hl=en-us',\n",
       " 'autohide=2',\n",
       " 'wmode=transparent',\n",
       " 'w=640',\n",
       " 'h=360',\n",
       " ']',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'lg',\n",
       " 'will',\n",
       " 'be',\n",
       " 'showcasing',\n",
       " 'it',\n",
       " 'own',\n",
       " 'update',\n",
       " 'though',\n",
       " 'they',\n",
       " 'have',\n",
       " 'already',\n",
       " 'shown',\n",
       " 'part',\n",
       " 'of',\n",
       " 'their',\n",
       " 'hand',\n",
       " 'by',\n",
       " 'releasing',\n",
       " 'the',\n",
       " 'detail',\n",
       " 'on',\n",
       " 'their',\n",
       " 'new',\n",
       " 'oled',\n",
       " 'tv',\n",
       " 'lineup',\n",
       " 'featuring',\n",
       " 'ai',\n",
       " 'processor',\n",
       " 'it',\n",
       " 'claim',\n",
       " 'will',\n",
       " 'significantly',\n",
       " 'improve',\n",
       " 'visual',\n",
       " 'and',\n",
       " 'audio',\n",
       " 'fidelity',\n",
       " 'over',\n",
       " 'prior',\n",
       " 'model',\n",
       " '.',\n",
       " 'lg',\n",
       " 'will',\n",
       " 'also',\n",
       " 'feature',\n",
       " 'update',\n",
       " 'on',\n",
       " 'home',\n",
       " 'mobility',\n",
       " 'and',\n",
       " 'you',\n",
       " 'guessed',\n",
       " 'it',\n",
       " 'ai',\n",
       " 'in',\n",
       " 'it',\n",
       " 'ce',\n",
       " 'event',\n",
       " '.',\n",
       " '[',\n",
       " 'youtube',\n",
       " 'http',\n",
       " ':',\n",
       " '//www.youtube.com/watch',\n",
       " 'v=5-96urqq-ue',\n",
       " 'version=3',\n",
       " 'rel=1',\n",
       " 'showsearch=0',\n",
       " 'showinfo=1',\n",
       " 'iv_load_policy=1',\n",
       " 'fs=1',\n",
       " 'hl=en-us',\n",
       " 'autohide=2',\n",
       " 'wmode=transparent',\n",
       " 'w=640',\n",
       " 'h=360',\n",
       " ']',\n",
       " 'the',\n",
       " 'appliance',\n",
       " 'and',\n",
       " 'electronics',\n",
       " 'manufacturer',\n",
       " 'is',\n",
       " 'making',\n",
       " 'it',\n",
       " 'automotive',\n",
       " 'debut',\n",
       " 'at',\n",
       " 'ce',\n",
       " 'with',\n",
       " 'an',\n",
       " 'in-vehicle',\n",
       " 'projection',\n",
       " 'system',\n",
       " '.',\n",
       " 'the',\n",
       " 'company',\n",
       " 'also',\n",
       " 'showcased',\n",
       " 'a',\n",
       " 'new',\n",
       " 'smart',\n",
       " 'built-in',\n",
       " 'dishwasher',\n",
       " 'with',\n",
       " '“',\n",
       " 'autodose',\n",
       " '”',\n",
       " 'and',\n",
       " '“',\n",
       " 'autodry',\n",
       " '”',\n",
       " 'feature',\n",
       " '.',\n",
       " '[',\n",
       " 'youtube',\n",
       " 'http',\n",
       " ':',\n",
       " '//www.youtube.com/watch',\n",
       " 'v=tmu1ivzgtwy',\n",
       " 'version=3',\n",
       " 'rel=1',\n",
       " 'showsearch=0',\n",
       " 'showinfo=1',\n",
       " 'iv_load_policy=1',\n",
       " 'fs=1',\n",
       " 'hl=en-us',\n",
       " 'autohide=2',\n",
       " 'wmode=transparent',\n",
       " 'w=640',\n",
       " 'h=360',\n",
       " ']',\n",
       " 'panasonic',\n",
       " 'is',\n",
       " 'leading',\n",
       " 'with',\n",
       " 'their',\n",
       " 'energy',\n",
       " 'and',\n",
       " 'climate',\n",
       " 'policy',\n",
       " 'in',\n",
       " 'a',\n",
       " 'break',\n",
       " 'from',\n",
       " 'the',\n",
       " 'other',\n",
       " 'company',\n",
       " 'keeping',\n",
       " 'a',\n",
       " 'big',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'ai',\n",
       " 'reveals',\n",
       " '.',\n",
       " '[',\n",
       " 'youtube',\n",
       " 'http',\n",
       " ':',\n",
       " '//www.youtube.com/watch',\n",
       " 'v=dq_rpa2od8o',\n",
       " 'version=3',\n",
       " 'rel=1',\n",
       " 'showsearch=0',\n",
       " 'showinfo=1',\n",
       " 'iv_load_policy=1',\n",
       " 'fs=1',\n",
       " 'hl=en-us',\n",
       " 'autohide=2',\n",
       " 'wmode=transparent',\n",
       " 'w=640',\n",
       " 'h=360',\n",
       " ']',\n",
       " 'the',\n",
       " 'electronics',\n",
       " 'company',\n",
       " 'unveiled',\n",
       " 'their',\n",
       " 'tcl',\n",
       " 'qd-mini',\n",
       " 'led',\n",
       " 'tv',\n",
       " 'full-color',\n",
       " 'rayneo',\n",
       " 'ar',\n",
       " 'glass',\n",
       " 'and',\n",
       " 'a',\n",
       " 'paper-like',\n",
       " 'display',\n",
       " 'optimized',\n",
       " 'for',\n",
       " 'human',\n",
       " 'eye',\n",
       " '.',\n",
       " '[',\n",
       " 'youtube',\n",
       " 'http',\n",
       " ':',\n",
       " '//www.youtube.com/watch',\n",
       " 'v=0y4wiqx5fnu',\n",
       " 'version=3',\n",
       " 'rel=1',\n",
       " 'showsearch=0',\n",
       " 'showinfo=1',\n",
       " 'iv_load_policy=1',\n",
       " 'fs=1',\n",
       " 'hl=en-us',\n",
       " 'autohide=2',\n",
       " 'wmode=transparent',\n",
       " 'w=640',\n",
       " 'h=360',\n",
       " ']',\n",
       " 'the',\n",
       " 'audio',\n",
       " 'company',\n",
       " 'sennheiser',\n",
       " 'will',\n",
       " 'have',\n",
       " 'their',\n",
       " 'own',\n",
       " 'ce',\n",
       " 'showcase',\n",
       " 'with',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'clear',\n",
       " 'focus',\n",
       " 'promising',\n",
       " 'new',\n",
       " 'headphone',\n",
       " 'announcement',\n",
       " 'from',\n",
       " 'their',\n",
       " 'live',\n",
       " 'stream',\n",
       " 'which',\n",
       " 'at',\n",
       " 'the',\n",
       " 'time',\n",
       " 'this',\n",
       " 'piece',\n",
       " 'wa',\n",
       " 'updated',\n",
       " 'had',\n",
       " 'been',\n",
       " 'taken',\n",
       " 'private',\n",
       " '.',\n",
       " '[',\n",
       " 'youtube',\n",
       " 'http',\n",
       " ':',\n",
       " '//www.youtube.com/watch',\n",
       " 'v=rvr1tthx8tq',\n",
       " 'version=3',\n",
       " 'rel=1',\n",
       " 'showsearch=0',\n",
       " 'showinfo=1',\n",
       " 'iv_load_policy=1',\n",
       " 'fs=1',\n",
       " 'hl=en-us',\n",
       " 'autohide=2',\n",
       " 'wmode=transparent',\n",
       " 'w=640',\n",
       " 'h=360',\n",
       " ']',\n",
       " 'hyundai',\n",
       " '’',\n",
       " 's',\n",
       " 'most',\n",
       " 'attention-grabbing',\n",
       " 'reveal',\n",
       " 'look',\n",
       " 'to',\n",
       " 'be',\n",
       " 'an',\n",
       " 'update',\n",
       " 'on',\n",
       " 'it',\n",
       " 'supernal',\n",
       " 'evtol',\n",
       " 'electric',\n",
       " 'vertical',\n",
       " 'takeoff',\n",
       " 'and',\n",
       " 'landing',\n",
       " 'vehicle',\n",
       " 'which',\n",
       " 'wa',\n",
       " 'first',\n",
       " 'showcased',\n",
       " 'back',\n",
       " 'at',\n",
       " 'ce',\n",
       " '2020.',\n",
       " 'in',\n",
       " 'addition',\n",
       " 'to',\n",
       " 'it',\n",
       " 'ce',\n",
       " 'kickoff',\n",
       " 'hyundai',\n",
       " 'is',\n",
       " 'hosting',\n",
       " 'separate',\n",
       " 'event',\n",
       " 'tuesday',\n",
       " 'focused',\n",
       " 'solely',\n",
       " 'on',\n",
       " 'the',\n",
       " 'evtol',\n",
       " 'concept',\n",
       " 'and',\n",
       " 'it',\n",
       " 'vision',\n",
       " 'for',\n",
       " 'mobility',\n",
       " 'hub',\n",
       " 'for',\n",
       " 'these',\n",
       " 'flying',\n",
       " 'vehicle',\n",
       " 'to',\n",
       " 'actually',\n",
       " 'take',\n",
       " 'off',\n",
       " 'and',\n",
       " 'land',\n",
       " 'from',\n",
       " '.',\n",
       " 'beyond',\n",
       " 'it',\n",
       " 'aerial',\n",
       " 'ambition',\n",
       " 'hyundai',\n",
       " 'will',\n",
       " 'be',\n",
       " 'talking',\n",
       " 'about',\n",
       " 'sustainability',\n",
       " 'software',\n",
       " 'and',\n",
       " 'of',\n",
       " 'course',\n",
       " 'ai',\n",
       " 'in',\n",
       " 'a',\n",
       " 'stream',\n",
       " 'you',\n",
       " 'can',\n",
       " 'watch',\n",
       " 'below',\n",
       " '.',\n",
       " '[',\n",
       " 'youtube',\n",
       " 'http',\n",
       " ':',\n",
       " '//www.youtube.com/watch',\n",
       " 'v=1jzpv3gtx1u',\n",
       " 'version=3',\n",
       " 'rel=1',\n",
       " 'showsearch=0',\n",
       " 'showinfo=1',\n",
       " 'iv_load_policy=1',\n",
       " 'fs=1',\n",
       " 'hl=en-us',\n",
       " 'autohide=2',\n",
       " 'wmode=transparent',\n",
       " 'w=640',\n",
       " 'h=360',\n",
       " ']',\n",
       " 'if',\n",
       " 'you',\n",
       " '’',\n",
       " 're',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'phone',\n",
       " 'news',\n",
       " 'from',\n",
       " 'samsung',\n",
       " 'you',\n",
       " '’',\n",
       " 'll',\n",
       " 'have',\n",
       " 'to',\n",
       " 'wait',\n",
       " 'until',\n",
       " 'january',\n",
       " '17',\n",
       " 'when',\n",
       " 'their',\n",
       " 'next',\n",
       " 'unpacked',\n",
       " 'event',\n",
       " 'will',\n",
       " 'kick',\n",
       " 'off',\n",
       " '.',\n",
       " 'a',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'the',\n",
       " 'case',\n",
       " 'for',\n",
       " 'several',\n",
       " 'year',\n",
       " 'samsung',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'their',\n",
       " 'product',\n",
       " 'line',\n",
       " 'at',\n",
       " 'ce',\n",
       " '2024.',\n",
       " 'and',\n",
       " 'those',\n",
       " 'product',\n",
       " 'are',\n",
       " 'about',\n",
       " 'to',\n",
       " 'get',\n",
       " 'the',\n",
       " 'ai',\n",
       " 'push',\n",
       " 'if',\n",
       " 'their',\n",
       " 'press',\n",
       " 'conference',\n",
       " 'title',\n",
       " '“',\n",
       " 'ai',\n",
       " 'for',\n",
       " 'all',\n",
       " ':',\n",
       " 'connectivity',\n",
       " 'in',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " 'ai',\n",
       " '”',\n",
       " 'wasn',\n",
       " '’',\n",
       " 't',\n",
       " 'enough',\n",
       " 'of',\n",
       " 'a',\n",
       " 'clue',\n",
       " '.',\n",
       " 'samsung',\n",
       " 'ha',\n",
       " 'already',\n",
       " 'revealed',\n",
       " 'some',\n",
       " 'ai',\n",
       " 'application',\n",
       " 'in',\n",
       " 'the',\n",
       " 'kitchen',\n",
       " 'and',\n",
       " 'in',\n",
       " 'it',\n",
       " 'updated',\n",
       " 'robot',\n",
       " 'vacuum',\n",
       " 'lineup',\n",
       " '.',\n",
       " 'but',\n",
       " 'it',\n",
       " 'expanded',\n",
       " 'that',\n",
       " 'lineup',\n",
       " 'to',\n",
       " 'include',\n",
       " 'a',\n",
       " 'fresh',\n",
       " 'look',\n",
       " 'at',\n",
       " 'it',\n",
       " 'ballie',\n",
       " 'home',\n",
       " 'assistant',\n",
       " 'robot',\n",
       " 'which',\n",
       " 'wa',\n",
       " 'initially',\n",
       " 'revealed',\n",
       " 'at',\n",
       " 'ce',\n",
       " '2020',\n",
       " 'a',\n",
       " 'well',\n",
       " 'a',\n",
       " 'a',\n",
       " 'series',\n",
       " 'of',\n",
       " 'update',\n",
       " 'to',\n",
       " 'it',\n",
       " 'smart',\n",
       " 'home',\n",
       " 'suite',\n",
       " 'to',\n",
       " 'include',\n",
       " 'household',\n",
       " 'map',\n",
       " '.',\n",
       " 'samsung',\n",
       " 'also',\n",
       " 'put',\n",
       " 'out',\n",
       " 'some',\n",
       " 'additional',\n",
       " 'tease',\n",
       " 'over',\n",
       " 'the',\n",
       " 'weekend',\n",
       " 'for',\n",
       " '“',\n",
       " 'new',\n",
       " 'generation',\n",
       " 'of',\n",
       " 'product',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'folded',\n",
       " 'inward',\n",
       " 'and',\n",
       " 'outward',\n",
       " '”',\n",
       " 'which',\n",
       " 'could',\n",
       " 'include',\n",
       " 'rollable',\n",
       " 'and',\n",
       " 'foldable',\n",
       " 'display',\n",
       " 'building',\n",
       " 'off',\n",
       " 'of',\n",
       " 'their',\n",
       " 'existing',\n",
       " 'line',\n",
       " 'of',\n",
       " 'foldable',\n",
       " 'phone',\n",
       " 'a',\n",
       " 'well',\n",
       " 'a',\n",
       " 'showing',\n",
       " 'off',\n",
       " 'a',\n",
       " 'transparent',\n",
       " 'microled',\n",
       " 'screen',\n",
       " 'at',\n",
       " 'an',\n",
       " 'event',\n",
       " 'at',\n",
       " 'ce',\n",
       " '.',\n",
       " ...]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea7ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
