{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ef6c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\endyj\\anaconda3\\envs\\py38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import string\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246b796",
   "metadata": {},
   "source": [
    "# Google Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dcd7fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "def get_useragent():\n",
    "    return random.choice(_useragent_list)\n",
    "\n",
    "_useragent_list = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36 Edg/111.0.1661.62',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0'\n",
    "]\n",
    "\n",
    "request_timeout = 10\n",
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:49.0) Gecko/20100101 Firefox/49.0',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Cache-Control': 'private, max-age=0',\n",
    "}\n",
    "html_pattern = \"<(?:\\\"[^\\\"]*\\\"['\\\"]*|'[^']*'['\\\"]*|[^'\\\">])+>\"\n",
    "TODAY = datetime.date.today()\n",
    "#TODAY = '2024-03-22'\n",
    "\n",
    "#search variables\n",
    "from_date = '1/1/2024'\n",
    "to_date = '1/3/2024'\n",
    "total_news = 20\n",
    "sleep_interval = 20\n",
    "stock_name = 'NVDA'\n",
    "\n",
    "#NLP variables\n",
    "model_id = \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ceba417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"googlesearch is a Python library for searching Google, easily.\"\"\"\n",
    "def get_headers(): \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:49.0) Gecko/20100101 Firefox/49.0',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Cache-Control': 'private, max-age=0',\n",
    "    }\n",
    "\n",
    "    return headers\n",
    "\n",
    "def _req(term, results, lang, start, proxies, timeout, from_date, to_date):\n",
    "    resp = get(\n",
    "        url=\"https://www.google.com/search\",\n",
    "        headers={\n",
    "            \"User-Agent\": get_useragent()\n",
    "        },\n",
    "        params={\n",
    "            \"q\": term,\n",
    "            \"num\": results,  # Prevents multiple requests\n",
    "            \"hl\": lang,\n",
    "            \"start\": start,\n",
    "            \"lr\": \"lang_en\",\n",
    "            \"tbm\": 'nws',\n",
    "            \"tbs\": f'sbd:1,cdr:1,cd_min:{from_date},cd_max:{to_date}'\n",
    "            #\"tbs\": \"qdr:d\"\n",
    "            #\"tbs\": f\"sbd:1,lr:lang_en,cdr:1,cd_min:1/1/{from_year},cd_max:{to_year}\"\n",
    "        },\n",
    "        proxies=proxies,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    return resp\n",
    "\n",
    "def search(term, num_results=10, lang=\"en\", proxy=None, advanced=False, sleep_interval=0, timeout=5, from_date='1/1/2020', to_date='1/1/2020'):\n",
    "    \"\"\"Search the Google search engine\"\"\"\n",
    "\n",
    "    escaped_term = urllib.parse.quote_plus(term) # make 'site:xxx.xxx.xxx ' works.\n",
    "\n",
    "    # Proxy\n",
    "    proxies = None\n",
    "    if proxy:\n",
    "        if proxy.startswith(\"https\"):\n",
    "            proxies = {\"https\": proxy}\n",
    "        else:\n",
    "            proxies = {\"http\": proxy}\n",
    "\n",
    "    # Fetch\n",
    "    start = 0\n",
    "    res = []\n",
    "    while start < num_results:\n",
    "        # Send request\n",
    "        resp = _req(escaped_term, num_results - start,\n",
    "                    lang, start, proxies, timeout, from_date, to_date)\n",
    "        # Parse\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        result_block = soup.find_all(\"div\", attrs={\"class\": \"SoaBEf\"})\n",
    "\n",
    "        if len(result_block)==0:\n",
    "            break\n",
    "\n",
    "        for result in result_block:\n",
    "            # Find url, title, description\n",
    "            url = result.find(\"a\", href=True)\n",
    "            title = result.find(\"div\", attrs={\"class\": \"n0jPhd ynAwRc MBeuO nDgy9d\"})\n",
    "            date = result.find(\"div\", attrs={\"class\": \"OSrXXb rbYSKb LfVVr\"})\n",
    "            obj = {}\n",
    "            obj[\"title\"] = None\n",
    "            obj[\"date\"] = None\n",
    "            obj[\"url\"] = None\n",
    "            start += 1\n",
    "\n",
    "            if title:\n",
    "                obj[\"title\"] = title.text\n",
    "            if date:\n",
    "                obj[\"date\"] = from_date\n",
    "            if url:\n",
    "                obj[\"url\"] = url[\"href\"]\n",
    "                res.append(obj)\n",
    "                        \n",
    "        sleep(sleep_interval)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fde9b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_news_urls(q, total, from_date, to_date, sleep_interval=0):\n",
    "    urls = []\n",
    "    urls = search(q, num_results=total, from_date=from_date, to_date=to_date, sleep_interval=sleep_interval)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b254fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_news_urls(q, total, from_date, to_date, temp_save=False):\n",
    "    dir_name = str(TODAY)\n",
    "    if dir_name not in os.listdir():\n",
    "        cwd = os.getcwd()\n",
    "        os.mkdir(f'{cwd}/{dir_name}')\n",
    "        \n",
    "    urls = get_google_news_urls(q, total, from_date, to_date, sleep_interval = 10)\n",
    "    # temp\n",
    "    if temp_save:\n",
    "        # replace date format to file savable characters\n",
    "        filename = f'{query}_{from_date.replace(\"/\", \"_\")}.json'\n",
    "        f = open(f'./{TODAY}/{filename}', 'w')\n",
    "        f.write(json.dumps(json_object, indent = 4))\n",
    "        f.close()\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f42a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_data(url):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        article = requests.get(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            timeout=request_timeout\n",
    "        )\n",
    "        status_code = article.status_code\n",
    "        if status_code >= 400:\n",
    "            f = open(f'./{TODAY}/ERROR_LOG.txt', 'a')\n",
    "            f.write(f'[{time.asctime(time.localtime())}] Code {status_code}: {url}\\n')\n",
    "            f.close()\n",
    "            return None\n",
    "        soup = bs(article.content, \"html.parser\")\n",
    "        article_body = soup.find(\"body\")\n",
    "        paragraphs = article_body.find_all(\"p\")\n",
    "        #sleep(1)\n",
    "        if paragraphs is not None:\n",
    "            for p in paragraphs:\n",
    "                text += re.sub(html_pattern, '', p.text).strip() + ' '  \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        msg = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef8e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(q, json_object, date):\n",
    "    total_count = 0\n",
    "    for data in json_object:  \n",
    "        try:\n",
    "            data['text'] = get_news_data(data['url'])\n",
    "            if data is None:\n",
    "                continue\n",
    "            if q not in os.listdir(f'./{TODAY}'):\n",
    "                cwd = os.getcwd()\n",
    "                os.mkdir(f'{cwd}/{TODAY}/{q}')\n",
    "            total_count += 1\n",
    "            file_error_symbols = []\n",
    "            filename = f'{total_count}_{date.replace(\"/\", \"_\")}'\n",
    "            f = open(f'./{TODAY}/{q}//{filename}.json', \"w\")\n",
    "            f.write(json.dumps(data, indent = 4))\n",
    "            f.close()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "    print(f'Saved {total_count} news data in {date}')\n",
    "    #print(os.system(\"npx prettier -w ./dataset/*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c56ef85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_from_file(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fdadc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_from_file(filename):\n",
    "    keywords = []\n",
    "    f = open(filename, \"r\")\n",
    "    for keyword in f:\n",
    "        keywords.append(keyword)\n",
    "    f.close()\n",
    "    return list(map(lambda keyword: keyword.strip(), keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "349e49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_save_dataset(q, urls):\n",
    "    start = time.time()\n",
    "    for url in urls:\n",
    "        print(urls)\n",
    "        save_dataset(q, urls[year], year)\n",
    "    #Parallel(n_jobs = -1)(delayed(save_dataset)(q, urls[year], year) for year in urls)\n",
    "    end = time.time()\n",
    "    print('{:.2f} seconds used'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deaf2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run all functions to save news data\n",
    "def scrap_news_data(total, from_date, to_date, keywords_filename=\"keywords.txt\"):\n",
    "    #count used time\n",
    "    start = time.time()\n",
    "    #step 1 - load keywords file\n",
    "    keywords = get_keywords_from_file(keywords_filename)\n",
    "    for query in keywords:\n",
    "        print(f'Scrapping {query} ...')\n",
    "        #step 2 - save urls to json file\n",
    "        save_news_urls(query, total, from_date, to_date)\n",
    "        #step 3 - get urls object from previous saved json file\n",
    "        urls = get_urls_from_file(f'./{TODAY}/{query}.json')\n",
    "        #step 4 - web scrapping news data from specific year of urls & save into directory seperately\n",
    "        save_dataset(query, urls, from_date)\n",
    "        #parallel_save_dataset(query, urls, date)\n",
    "        #save_dataset(query, urls)\n",
    "        \n",
    "    end = time.time()\n",
    "    print('[DONE] {:.2f} seconds used'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84a37454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv(q, json_obj, index):\n",
    "    text = None\n",
    "    title = None\n",
    "    date = None\n",
    "    url = json_obj['url']\n",
    "    if 'date' in json_obj.keys():\n",
    "        date = json_obj['date']\n",
    "    if json_obj['text'] is not None:\n",
    "        text = re.sub(r'[,.\\'\"|]', '', json_obj['text'])\n",
    "    if 'title' in json_obj.keys() and json_obj['title'] != None:\n",
    "        title = re.sub(r'[,.\\'\"|]', '', json_obj['title'])\n",
    "    if text != '' and text != None:\n",
    "        text = ' '.join(text.encode('utf-8').decode().split())\n",
    "    if title != '' and title != None:\n",
    "        title = ' '.join(title.split()).strip() \n",
    "    f = open(f'./{TODAY}/{q}.csv', 'a', encoding='utf-8')\n",
    "    f.write(f'{index},\\\"{title}\\\",\\\"{date}\\\",\\\"{url}\\\",\\\"{text}\\\"\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49f6fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date_time_format(date):\n",
    "    date = date.split('/')\n",
    "    date = list(map(lambda x: int(x), date))\n",
    "    return datetime.datetime(date[2], date[1], date[0])\n",
    "\n",
    "def to_google_param_format(date):\n",
    "    return f'{date.month}/{date.day}/{date.year}'\n",
    "\n",
    "def save_json_object(filename, json_object):\n",
    "    f = open(f'./{TODAY}/{filename}', 'w')\n",
    "    f.write(json.dumps(json_object, indent = 4))\n",
    "    f.close()\n",
    "    \n",
    "def get_date_range_list(from_date, to_date):\n",
    "    from_date_time = to_date_time_format(from_date)\n",
    "    to_date_time = to_date_time_format(to_date)\n",
    "    day_difference = (to_date_time - from_date_time).days\n",
    "    datetime_list = [from_date_time + datetime.timedelta(days=x) for x in range(day_difference + 1)]\n",
    "    date_range_list = list(map(lambda x: to_google_param_format(x), datetime_list))\n",
    "    return date_range_list\n",
    "    \n",
    "def parallel_save_dataset(query, from_date, to_date):\n",
    "    # count used time\n",
    "    start = time.time()\n",
    "    # JSON object\n",
    "    json_object = dict()\n",
    "    # get from date & to date difference days\n",
    "    date_range_list = get_date_range_list(from_date, to_date)\n",
    "    # iterate through date range list\n",
    "    for date in date_range_list:\n",
    "        urls = save_news_urls(query, total_news, date, date)\n",
    "        json_object[date] = urls\n",
    "        \n",
    "    # replace date format to file savable characters\n",
    "    filename = f'{query}.json'\n",
    "    # temporary save of the scrapped urls\n",
    "    save_json_object(filename, json_object)\n",
    "    # parallel computing to save news data\n",
    "    Parallel(n_jobs = -1)(delayed(save_dataset)(query, json_object[date], date) for date in json_object)\n",
    "    #for date in json_object:\n",
    "    #    save_dataset(query, json_object[date], date)\n",
    "    # convert all json file in single csv file\n",
    "    all_json_to_csv(stock_name)\n",
    "    print(f'{filename}: [DONE]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0761ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_json_to_csv(stock_name):\n",
    "    cols = \"title,date,url,text\\n\"\n",
    "    keywords = get_keywords_from_file('keywords.txt')\n",
    "    for keyword in keywords:\n",
    "        f = open(f'./{TODAY}/{stock_name}.csv', 'w')\n",
    "        f.write(cols)\n",
    "        f.close()\n",
    "        for index, filename in enumerate(os.listdir(f'./{TODAY}/{stock_name}')):\n",
    "            if '.json' not in filename:\n",
    "                continue\n",
    "            json_file = open(f'./{TODAY}/{stock_name}/{filename}', 'r')\n",
    "            json_obj = json.load(json_file)\n",
    "            json_file.close()\n",
    "            json_to_csv(stock_name, json_obj, index)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95b07911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_text(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    special_characters = \"!@#$%^&*()-+?_=,<>\\\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_words = word_tokenize(text.lower())\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_words]\n",
    "    preprocessed_words = [word for word in lemmatized_words if word not in stop_words]\n",
    "    preprocessed_words = [word for word in lemmatized_words if word not in special_characters]\n",
    "    preprocessed_text = \" \".join(preprocessed_words)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0441cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_sentiment_score(news_df):\n",
    "    sentiment_obj = dict()\n",
    "    sentiment_score_df = []\n",
    "    news_df_length = len(news_df)\n",
    "    \n",
    "    for i in range(news_df_length):\n",
    "        data = news_df.iloc[i]\n",
    "        date = data['date']\n",
    "        title = data['title']\n",
    "        text = data['text']\n",
    "        preprocessed_text = get_preprocessed_text(text)\n",
    "        sentiment_result = sentiment_pipe(preprocessed_text)[0]\n",
    "        score = sentiment_result['score']\n",
    "        label = sentiment_result['label']\n",
    "        \n",
    "        if date not in sentiment_obj.keys():\n",
    "            sentiment_obj[date] = dict()\n",
    "            sentiment_obj[date]['score'] = 0\n",
    "            sentiment_obj[date]['count'] = 0\n",
    "        if label != 'neutral': \n",
    "            if label == 'positive':\n",
    "                sentiment_obj[date]['score'] += score\n",
    "            else:\n",
    "                sentiment_obj[date]['score'] -= score\n",
    "            sentiment_obj[date]['count'] += 1\n",
    "    \n",
    "    for date in sentiment_obj:\n",
    "        date_list = list(map(lambda x: int(x), date.split('/')))\n",
    "        datetime_obj = datetime.datetime(date_list[2], date_list[0], date_list[1])\n",
    "        score = sentiment_obj[date]['score']\n",
    "        count = sentiment_obj[date]['count']\n",
    "        if count > 0:\n",
    "            item = [datetime_obj,  score / count]\n",
    "            sentiment_score_df.append(item)\n",
    "        \n",
    "    sentiment_score_df = sorted(sentiment_score_df, key=lambda x: x[0])\n",
    "    sentiment_score_df = list(map(lambda x: [str(x[0].date()), x[1]], sentiment_score_df))\n",
    "    sentiment_score_df = pd.DataFrame(sentiment_score_df, columns =['Date', 'Score'], dtype = float) \n",
    "    return sentiment_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06dc6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movement_accuracy(stock_df, sentiment_df):\n",
    "    stock_available_dates = stock_df['Date'].values\n",
    "    date_length = len(stock_available_dates)\n",
    "    stock_val_list = stock_df['Adj Close'].values\n",
    "    sentiment_date_list = sentiment_df['Date'].values\n",
    "    \n",
    "    real_label_list = []\n",
    "    predict_label_list = []\n",
    "    \n",
    "    for i in range(len(sentiment_date_list)):\n",
    "        if sentiment_date_list[i] not in stock_available_dates:\n",
    "            sentiment_df = sentiment_df.drop(i)\n",
    "    \n",
    "    sentiment_score_list = sentiment_df['Score'].values\n",
    "\n",
    "    for i in range(1, date_length):\n",
    "        real_diff = stock_val_list[i] - stock_val_list[i-1]\n",
    "        sentiment_diff = sentiment_score_list[i] - sentiment_score_list[i-1]\n",
    "        \n",
    "        if real_diff > 0:\n",
    "            label = 'up'\n",
    "        else:\n",
    "            label = 'drop'\n",
    "            \n",
    "        real_label_list.append(label)\n",
    "        \n",
    "        if sentiment_diff > 0:\n",
    "            label = 'up'\n",
    "        else:\n",
    "            label = 'drop'\n",
    "            \n",
    "        predict_label_list.append(label)\n",
    "        \n",
    "    # compare the movement accuracy\n",
    "    label_list_length = len(real_label_list)\n",
    "    total = 0\n",
    "    for i in range(label_list_length):\n",
    "        if real_label_list[i] == predict_label_list[i]:\n",
    "            total += 1\n",
    "    accuracy = total / date_length\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c52fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(index_arr, score_arr):\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    ax.plot(index_arr, score_arr)\n",
    "    ax.set_title(f'AAPL Sentiment Score')\n",
    "    ax.set_xlabel(f'Date')\n",
    "    ax.set_ylabel(f'Score')\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(base=10))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(base=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01fc43d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "NVDA.json: [DONE]\n"
     ]
    }
   ],
   "source": [
    "parallel_save_dataset(stock_name, from_date, to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28cfe03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1/10/2024</td>\n",
       "      <td>https://investorplace.com/market360/2024/01/op...</td>\n",
       "      <td>Countdown to Luke Lango’s “2nd Trillion-Dollar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Datacenter GPU Gravy Train That No One Wil...</td>\n",
       "      <td>1/11/2024</td>\n",
       "      <td>https://www.nextplatform.com/2024/01/11/the-da...</td>\n",
       "      <td>We have five decades of very fine-grained anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Believe It Or Not Nvidia Stock Is Still Cheap ...</td>\n",
       "      <td>1/12/2024</td>\n",
       "      <td>https://seekingalpha.com/article/4662529-belie...</td>\n",
       "      <td>Antonio Bordunovi Antonio Bordunovi Nvidia Cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Forget Nvidia Heres a Better Magnificent Seven...</td>\n",
       "      <td>1/13/2024</td>\n",
       "      <td>https://www.fool.com/investing/2024/01/13/forg...</td>\n",
       "      <td>Founded in 1993 The Motley Fool is a financial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NVIDIA Corp (NVDA)s Winning Formula: Financial...</td>\n",
       "      <td>1/15/2024</td>\n",
       "      <td>https://www.gurufocus.com/news/2153449/nvidia-...</td>\n",
       "      <td>NVIDIA Corp (NVDA Financial) has recently been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>Should You Buy Nvidia After Its Huge Run? ETFs...</td>\n",
       "      <td>2/6/2024</td>\n",
       "      <td>https://www.zacks.com/stock/news/2221565/shoul...</td>\n",
       "      <td>We use cookies to understand how you use our s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>AMD Broadcom Marvell Nvidia: Big gainers from ...</td>\n",
       "      <td>2/7/2024</td>\n",
       "      <td>https://www.marketbeat.com/originals/amd-broad...</td>\n",
       "      <td>A new Bank of America report found that data c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2/8/2024</td>\n",
       "      <td>https://www.investors.com/etfs-and-funds/secto...</td>\n",
       "      <td>BREAKING: Futures Fall Slightly Just when you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>Morning Brew: Nvidia Sets Up New AI Chip Unit ...</td>\n",
       "      <td>2/9/2024</td>\n",
       "      <td>https://www.gurufocus.com/news/2225804/morning...</td>\n",
       "      <td>Amid a rapidly evolving technological landscap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>Stocks climb yields fall as data supports rate...</td>\n",
       "      <td>3/1/2024</td>\n",
       "      <td>https://www.marketscreener.com/quote/stock/NVI...</td>\n",
       "      <td>Log In English (USA) English (UK) English (Can...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>975 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title       date  \\\n",
       "0                                                   NaN  1/10/2024   \n",
       "1     The Datacenter GPU Gravy Train That No One Wil...  1/11/2024   \n",
       "2     Believe It Or Not Nvidia Stock Is Still Cheap ...  1/12/2024   \n",
       "3     Forget Nvidia Heres a Better Magnificent Seven...  1/13/2024   \n",
       "5     NVIDIA Corp (NVDA)s Winning Formula: Financial...  1/15/2024   \n",
       "...                                                 ...        ...   \n",
       "1115  Should You Buy Nvidia After Its Huge Run? ETFs...   2/6/2024   \n",
       "1116  AMD Broadcom Marvell Nvidia: Big gainers from ...   2/7/2024   \n",
       "1117                                                NaN   2/8/2024   \n",
       "1118  Morning Brew: Nvidia Sets Up New AI Chip Unit ...   2/9/2024   \n",
       "1119  Stocks climb yields fall as data supports rate...   3/1/2024   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://investorplace.com/market360/2024/01/op...   \n",
       "1     https://www.nextplatform.com/2024/01/11/the-da...   \n",
       "2     https://seekingalpha.com/article/4662529-belie...   \n",
       "3     https://www.fool.com/investing/2024/01/13/forg...   \n",
       "5     https://www.gurufocus.com/news/2153449/nvidia-...   \n",
       "...                                                 ...   \n",
       "1115  https://www.zacks.com/stock/news/2221565/shoul...   \n",
       "1116  https://www.marketbeat.com/originals/amd-broad...   \n",
       "1117  https://www.investors.com/etfs-and-funds/secto...   \n",
       "1118  https://www.gurufocus.com/news/2225804/morning...   \n",
       "1119  https://www.marketscreener.com/quote/stock/NVI...   \n",
       "\n",
       "                                                   text  \n",
       "0     Countdown to Luke Lango’s “2nd Trillion-Dollar...  \n",
       "1     We have five decades of very fine-grained anal...  \n",
       "2     Antonio Bordunovi Antonio Bordunovi Nvidia Cor...  \n",
       "3     Founded in 1993 The Motley Fool is a financial...  \n",
       "5     NVIDIA Corp (NVDA Financial) has recently been...  \n",
       "...                                                 ...  \n",
       "1115  We use cookies to understand how you use our s...  \n",
       "1116  A new Bank of America report found that data c...  \n",
       "1117  BREAKING: Futures Fall Slightly Just when you ...  \n",
       "1118  Amid a rapidly evolving technological landscap...  \n",
       "1119  Log In English (USA) English (UK) English (Can...  \n",
       "\n",
       "[975 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.read_csv(f\"./{TODAY}/{stock_name}.csv\")\n",
    "news_df = news_df.mask(news_df.eq('None')).dropna(subset=['text'])\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75a956e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "stock_df = yf.download(stock_name, start=f'2024-01-01', end=f'2024-03-01')\n",
    "stock_df = stock_df.reset_index()\n",
    "stock_df['Date'] = stock_df['Date'].apply(lambda x: str(x.date()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3577ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1196 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "sentiment_df = get_weighted_sentiment_score(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f4e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df.to_csv(f'./{TODAY}/{stock_name}_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae2db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = get_movement_accuracy(stock_df, sentiment_df)\n",
    "print(f'{stock_name} Sentiment Analysis Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13670b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
