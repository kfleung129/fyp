{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ef6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import string\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246b796",
   "metadata": {},
   "source": [
    "# Google Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9dcd7fb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'method_descriptor' object has no attribute 'today'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 24\u001b[0m\n\u001b[0;32m     15\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 6.1; WOW64; rv:49.0) Gecko/20100101 Firefox/49.0\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccept\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCache-Control\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprivate, max-age=0\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     22\u001b[0m }\n\u001b[0;32m     23\u001b[0m html_pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<(?:\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m]*\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m]*|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m]*|[^\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m>])+>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 24\u001b[0m TODAY \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoday\u001b[49m()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#search variables\u001b[39;00m\n\u001b[0;32m     27\u001b[0m from_year \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2024\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'method_descriptor' object has no attribute 'today'"
     ]
    }
   ],
   "source": [
    "def get_useragent():\n",
    "    return random.choice(_useragent_list)\n",
    "\n",
    "_useragent_list = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36 Edg/111.0.1661.62',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0'\n",
    "]\n",
    "\n",
    "request_timeout = 10\n",
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:49.0) Gecko/20100101 Firefox/49.0',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Cache-Control': 'private, max-age=0',\n",
    "}\n",
    "html_pattern = \"<(?:\\\"[^\\\"]*\\\"['\\\"]*|'[^']*'['\\\"]*|[^'\\\">])+>\"\n",
    "TODAY = datetime.date.today()\n",
    "\n",
    "#search variables\n",
    "from_year = 2024\n",
    "to_year = 2024\n",
    "total_news_per_year = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6ceba417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"googlesearch is a Python library for searching Google, easily.\"\"\"\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import urllib\n",
    "import random\n",
    "\n",
    "def get_headers(): \n",
    "    headers = {\n",
    "        'User-Agent': get_useragent(),\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Cache-Control': 'private, max-age=0',\n",
    "    }\n",
    "\n",
    "    return headers\n",
    "\n",
    "def _req(term, results, lang, start, proxies, timeout, from_year, to_year):\n",
    "    resp = get(\n",
    "        url=\"https://www.google.com/search\",\n",
    "        headers={\n",
    "            \"User-Agent\": get_useragent()\n",
    "        },\n",
    "        params={\n",
    "            \"q\": term,\n",
    "            \"num\": results,  # Prevents multiple requests\n",
    "            \"hl\": lang,\n",
    "            \"start\": start,\n",
    "            \"lr\": \"lang_en\",\n",
    "            \"tbm\": 'nws',\n",
    "            \"tbs\": 'sbd:1'\n",
    "            #\"tbs\": \"qdr:d\"\n",
    "            #\"tbs\": f\"sbd:1,lr:lang_en,cdr:1,cd_min:1/1/{from_year},cd_max:{to_year}\"\n",
    "        },\n",
    "        proxies=proxies,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    return resp\n",
    "\n",
    "def search(term, num_results=10, lang=\"en\", proxy=None, advanced=False, sleep_interval=0, timeout=5, from_year=2020, to_year=2020):\n",
    "    \"\"\"Search the Google search engine\"\"\"\n",
    "\n",
    "    escaped_term = urllib.parse.quote_plus(term) # make 'site:xxx.xxx.xxx ' works.\n",
    "\n",
    "    # Proxy\n",
    "    proxies = None\n",
    "    if proxy:\n",
    "        if proxy.startswith(\"https\"):\n",
    "            proxies = {\"https\": proxy}\n",
    "        else:\n",
    "            proxies = {\"http\": proxy}\n",
    "\n",
    "    # Fetch\n",
    "    start = 0\n",
    "    res = []\n",
    "    while start < num_results:\n",
    "        # Send request\n",
    "        resp = _req(escaped_term, num_results - start,\n",
    "                    lang, start, proxies, timeout, from_year, to_year)\n",
    "        # Parse\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        result_block = soup.find_all(\"div\", attrs={\"class\": \"SoaBEf\"})\n",
    "\n",
    "        if len(result_block)==0:\n",
    "            break\n",
    "\n",
    "        for result in result_block:\n",
    "            # Find url, title, description\n",
    "            url = result.find(\"a\", href=True)\n",
    "            title = result.find(\"div\", attrs={\"class\": \"n0jPhd ynAwRc MBeuO nDgy9d\"})\n",
    "            date = result.find(\"div\", attrs={\"class\": \"OSrXXb rbYSKb LfVVr\"})\n",
    "            obj = {}\n",
    "            obj[\"title\"] = None\n",
    "            obj[\"date\"] = None\n",
    "            obj[\"url\"] = None\n",
    "            start += 1\n",
    "\n",
    "            if title:\n",
    "                obj[\"title\"] = title.text\n",
    "            if date:\n",
    "                obj[\"date\"] = date.text\n",
    "            if url:\n",
    "                obj[\"url\"] = url[\"href\"]\n",
    "                res.append(obj)\n",
    "                        \n",
    "        sleep(sleep_interval)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2fde9b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_news_urls(q, total, from_year, to_year, sleep_interval=0):\n",
    "    urls = []\n",
    "    urls = search(q, num_results=total, from_year=from_year, to_year=to_year, sleep_interval=sleep_interval)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b254fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_news_urls(q, total, from_year, to_year):\n",
    "    dir_name = str(TODAY)\n",
    "    filename = f'{q}.json'\n",
    "    \n",
    "    if dir_name not in os.listdir():\n",
    "        cwd = os.getcwd()\n",
    "        os.mkdir(f'{cwd}/{dir_name}')\n",
    "        \n",
    "    if filename in os.listdir(f'./{dir_name}'):\n",
    "        print(f'{q}.json exists.')\n",
    "        return None\n",
    "    \n",
    "    urls = {}\n",
    "    for year in range(from_year, to_year + 1):\n",
    "        urls[str(year)] = get_google_news_urls(q, total, year, year)\n",
    "        print(f'Year {year} News urls collection - Done')\n",
    "        #delay request to prevent 429 Too many Request\n",
    "    print()\n",
    "    \n",
    "    f = open(f'./{TODAY}/{filename}', 'w')\n",
    "    f.write(json.dumps(urls, indent = 4))\n",
    "    f.close()\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "91f42a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_data(url):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        article = requests.get(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            timeout=request_timeout\n",
    "        )\n",
    "        status_code = article.status_code\n",
    "        if status_code >= 400:\n",
    "            f = open(f'./{TODAY}/ERROR_LOG.txt', 'a')\n",
    "            f.write(f'[{time.asctime(time.localtime())}] Code {status_code}: {url}\\n')\n",
    "            f.close()\n",
    "            return None\n",
    "        soup = bs(article.content, \"html.parser\")\n",
    "        article_body = soup.find(\"body\")\n",
    "        paragraphs = article_body.find_all(\"p\")\n",
    "        #sleep(1)\n",
    "        if paragraphs is not None:\n",
    "            for p in paragraphs:\n",
    "                text += re.sub(html_pattern, '', p.text).strip() + ' '  \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        msg = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef8e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(q, urls, year):\n",
    "    total_count = 0\n",
    "    for url in urls:\n",
    "        try:\n",
    "            data = url\n",
    "            data['text'] = get_news_data(data['url'])\n",
    "            if data is None:\n",
    "                continue\n",
    "            if q not in os.listdir(f'./{TODAY}'):\n",
    "                cwd = os.getcwd()\n",
    "                os.mkdir(f'{cwd}/{TODAY}/{q}')\n",
    "            total_count += 1\n",
    "            file_error_symbols = []\n",
    "            filename = f'{year}-{total_count}'\n",
    "            f = open(f'./{TODAY}/{q}/{filename}.json', \"w\")\n",
    "            f.write(json.dumps(data, indent = 4))\n",
    "            f.close()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "    print(f'Saved {total_count} news data in year {year}')\n",
    "    #print(os.system(\"npx prettier -w ./dataset/*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c56ef85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_from_file(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fdadc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_from_file(filename):\n",
    "    keywords = []\n",
    "    f = open(filename, \"r\")\n",
    "    for keyword in f:\n",
    "        keywords.append(keyword)\n",
    "    f.close()\n",
    "    return list(map(lambda keyword: keyword.strip(), keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "349e49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_save_dataset(q, urls):\n",
    "    start = time.time()\n",
    "    for year in urls:\n",
    "        save_dataset(q, urls[year], year)\n",
    "    #Parallel(n_jobs = -1)(delayed(save_dataset)(q, urls[year], year) for year in urls)\n",
    "    end = time.time()\n",
    "    print('{:.2f} seconds used'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "deaf2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run all functions to save news data\n",
    "def scrap_news_data(total, from_year, to_year, keywords_filename=\"keywords.txt\"):\n",
    "    #count used time\n",
    "    start = time.time()\n",
    "    #step 1 - load keywords file\n",
    "    keywords = get_keywords_from_file(keywords_filename)\n",
    "    for query in keywords:\n",
    "        print(f'Scrapping {query} ...')\n",
    "        #step 2 - save urls to json file\n",
    "        save_news_urls(query, total, from_year, to_year)\n",
    "        #step 3 - get urls object from previous saved json file\n",
    "        urls = get_urls_from_file(f'./{TODAY}/{query}.json')\n",
    "        #step 4 - web scrapping news data from specific year of urls & save into directory seperately\n",
    "        parallel_save_dataset(query, urls)\n",
    "        #save_dataset(query, urls)\n",
    "        \n",
    "    end = time.time()\n",
    "    print('[DONE] {:.2f} seconds used'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84a37454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv(q, json_obj, index):\n",
    "    text = None\n",
    "    title = None\n",
    "    date = None\n",
    "    url = json_obj['url']\n",
    "    if 'date' in json_obj.keys():\n",
    "        date = json_obj['date']\n",
    "    if json_obj['text'] is not None:\n",
    "        text = re.sub(r'[,.\\'\"|]', '', json_obj['text'])\n",
    "    if 'title' in json_obj.keys() and json_obj['title'] != None:\n",
    "        title = re.sub(r'[,.\\'\"|]', '', json_obj['title'])\n",
    "    if text != '' and text != None:\n",
    "        text = ' '.join(text.encode('utf-8').decode().split())\n",
    "    if title != '' and title != None:\n",
    "        title = ' '.join(title.split()).strip() \n",
    "    f = open(f'./{TODAY}/{q}.csv', 'a', encoding='utf-8')\n",
    "    f.write(f'{index},\\\"{title}\\\",\\\"{date}\\\",\\\"{url}\\\",\\\"{text}\\\"\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0761ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_json_to_csv():\n",
    "    cols = \"title,date,url,text\\n\"\n",
    "    keywords = get_keywords_from_file('keywords.txt')\n",
    "    for keyword in keywords:\n",
    "        f = open(f'./{TODAY}/{keyword}.csv', 'w')\n",
    "        f.write(cols)\n",
    "        f.close()\n",
    "        for index, filename in enumerate(os.listdir(f'./{TODAY}/{keyword}')):\n",
    "            json_file = open(f'./{TODAY}/{keyword}/{filename}', 'r')\n",
    "            json_obj = json.load(json_file)\n",
    "            json_file.close()\n",
    "            json_to_csv(keyword, json_obj, index)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "eb9ddf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv():\n",
    "    keywords = get_keywords_from_file('keywords.txt')\n",
    "    dataframes = []\n",
    "    for keyword in keywords:\n",
    "        df = pd.read_csv(f'./{TODAY}/{keyword}.csv')\n",
    "        dataframes.append(df)\n",
    "    dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "    # drop duplicate url\n",
    "    dataframe = dataframe.drop_duplicates(subset=['url'])\n",
    "    dataframe.to_csv(f'./{TODAY}/dataset.csv')\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "95b07911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_text(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    special_characters = \"!@#$%^&*()-+?_=,<>\\\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_words = word_tokenize(text.lower())\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_words]\n",
    "    preprocessed_words = [word for word in lemmatized_words if word not in stop_words]\n",
    "    preprocessed_words = [word for word in lemmatized_words if word not in special_characters]\n",
    "    preprocessed_text = \" \".join(preprocessed_words)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ba3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_sentiment_score(stock_name):\n",
    "    \n",
    "    scrap_news_data(total_news_per_year, from_year, to_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e0568eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping AAPL ...\n",
      "Year 2024 News urls collection - Done\n",
      "\n",
      "Saved 10 news data in year 2024\n",
      "10.46 seconds used\n",
      "Scrapping MSFT ...\n",
      "Year 2024 News urls collection - Done\n",
      "\n",
      "Saved 10 news data in year 2024\n",
      "8.40 seconds used\n",
      "Scrapping NVDA ...\n",
      "NVDA.json exists.\n",
      "Saved 10 news data in year 2024\n",
      "18.33 seconds used\n",
      "Scrapping AMZN ...\n",
      "Year 2024 News urls collection - Done\n",
      "\n",
      "Saved 10 news data in year 2024\n",
      "16.38 seconds used\n",
      "[DONE] 56.35 seconds used\n"
     ]
    }
   ],
   "source": [
    "scrap_news_data(total_news_per_year, from_year, to_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e6b7d35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "all_json_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bb12f78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "combine_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "28cfe03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jim Cramer Likes Jeff Bezos Amazon Over Shopif...</td>\n",
       "      <td>37 minutes ago</td>\n",
       "      <td>https://markets.businessinsider.com/news/stock...</td>\n",
       "      <td>In a recent segment of CNBC’s “Mad Money” reno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reddits IPO Ether investigation and Amazons “B...</td>\n",
       "      <td>43 minutes ago</td>\n",
       "      <td>https://qz.com/emails/daily-brief/1851353923/r...</td>\n",
       "      <td>Fund next-gen business journalism with $10 a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazoncom Inc (NASDAQ:AMZN) Position Lifted by...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>https://www.marketbeat.com/instant-alerts/nasd...</td>\n",
       "      <td>Benson Investment Management Company Inc raise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can This Beaten-Down Stock Skyrocket More Than...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>https://www.theglobeandmail.com/investing/mark...</td>\n",
       "      <td>The Magnificent Seven refers to seven of the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3 Billionaires Are Selling Artificial Intellig...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>https://www.theglobeandmail.com/investing/mark...</td>\n",
       "      <td>For three decades investors have been privy to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>iRobot Shows Just How Risky Investing in Merge...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>https://www.theglobeandmail.com/investing/mark...</td>\n",
       "      <td>Theres an investment tactic on Wall Street cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Amazon benefits from stellar Astera Labs debut...</td>\n",
       "      <td>2 hours ago</td>\n",
       "      <td>https://www.proactiveinvestors.com/companies/n...</td>\n",
       "      <td>This website uses cookies so that we can provi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Meet Wall Streets Newest Stock-Split Stock Alo...</td>\n",
       "      <td>2 hours ago</td>\n",
       "      <td>https://www.theglobeandmail.com/investing/mark...</td>\n",
       "      <td>Volatility is a given when putting your money ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How Amazon Relay in the UK enables women in th...</td>\n",
       "      <td>3 hours ago</td>\n",
       "      <td>https://www.aboutamazon.eu/news/transportation...</td>\n",
       "      <td>This content is hosted by a third party (wwwyo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title            date  \\\n",
       "0  Jim Cramer Likes Jeff Bezos Amazon Over Shopif...  37 minutes ago   \n",
       "2  Reddits IPO Ether investigation and Amazons “B...  43 minutes ago   \n",
       "3  Amazoncom Inc (NASDAQ:AMZN) Position Lifted by...      1 hour ago   \n",
       "4  Can This Beaten-Down Stock Skyrocket More Than...      1 hour ago   \n",
       "5  3 Billionaires Are Selling Artificial Intellig...      1 hour ago   \n",
       "6  iRobot Shows Just How Risky Investing in Merge...      1 hour ago   \n",
       "7  Amazon benefits from stellar Astera Labs debut...     2 hours ago   \n",
       "8  Meet Wall Streets Newest Stock-Split Stock Alo...     2 hours ago   \n",
       "9  How Amazon Relay in the UK enables women in th...     3 hours ago   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://markets.businessinsider.com/news/stock...   \n",
       "2  https://qz.com/emails/daily-brief/1851353923/r...   \n",
       "3  https://www.marketbeat.com/instant-alerts/nasd...   \n",
       "4  https://www.theglobeandmail.com/investing/mark...   \n",
       "5  https://www.theglobeandmail.com/investing/mark...   \n",
       "6  https://www.theglobeandmail.com/investing/mark...   \n",
       "7  https://www.proactiveinvestors.com/companies/n...   \n",
       "8  https://www.theglobeandmail.com/investing/mark...   \n",
       "9  https://www.aboutamazon.eu/news/transportation...   \n",
       "\n",
       "                                                text  \n",
       "0  In a recent segment of CNBC’s “Mad Money” reno...  \n",
       "2  Fund next-gen business journalism with $10 a m...  \n",
       "3  Benson Investment Management Company Inc raise...  \n",
       "4  The Magnificent Seven refers to seven of the l...  \n",
       "5  For three decades investors have been privy to...  \n",
       "6  Theres an investment tactic on Wall Street cal...  \n",
       "7  This website uses cookies so that we can provi...  \n",
       "8  Volatility is a given when putting your money ...  \n",
       "9  This content is hosted by a third party (wwwyo...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv(f\"./{TODAY}/AMZN.csv\").dropna()\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2bde8b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    Reddits IPO Ether investigation and Amazons “B...\n",
       "date                                        43 minutes ago\n",
       "url      https://qz.com/emails/daily-brief/1851353923/r...\n",
       "text     Fund next-gen business journalism with $10 a m...\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0441cb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jim Cramer Likes Jef...: [positive] 0.9983135461807251\n",
      "Reddits IPO Ether in...: [negative] 0.7878830432891846\n",
      "Amazoncom Inc (NASDA...: [neutral] 0.9254025816917419\n",
      "Can This Beaten-Down...: [negative] 0.9904794096946716\n",
      "3 Billionaires Are S...: [positive] 0.5940026640892029\n",
      "iRobot Shows Just Ho...: [neutral] 0.9040579199790955\n",
      "Amazon benefits from...: [neutral] 0.9992051720619202\n",
      "Meet Wall Streets Ne...: [neutral] 0.7089603543281555\n",
      "How Amazon Relay in ...: [positive] 0.9726082682609558\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(news)):\n",
    "    data = news.iloc[i]\n",
    "    title = data['title']\n",
    "    text = data['text']\n",
    "    preprocessed_text = get_preprocessed_text(text)\n",
    "    sentiment_result = sentiment_pipe(preprocessed_text)[0]\n",
    "    print(f\"{title[0:20]}...: [{sentiment_result['label']}] {sentiment_result['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caef588a",
   "metadata": {},
   "source": [
    "# Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5595d84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [404]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"https://finance.yahoo.com/quote/NVDA/news?p=NVDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3103f2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\endyj\\anaconda3\\envs\\py38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "\n",
    "nvda = yf.Ticker(\"NVDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9c2737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = nvda.news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "370f38fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-21 11:22:48 Isn’t Nvidia (NASDAQ:NVDA) Stock Overvalued? Not Quite\n",
      "2024-03-21 11:08:39 Tech Fund Beating 99% of Peers Says AI Rally Is Just Starting\n",
      "2024-03-21 06:33:00 Analysts revamp Super Micro Computer stock target after share offering\n",
      "2024-03-21 05:38:52 How Micron can remain competitive with Korean peers: Analyst\n",
      "2024-03-21 05:11:00 Nvidia Is Using Its Old 1990s Playbook To Best Its AI Rivals\n",
      "2024-03-21 04:48:07 Nvidia Pricing New AI Chips Aggressively To Maintain Market Share\n",
      "2024-03-21 04:42:08 Nvidia Is One Of 2 Mag 7 Stocks On This Breakout Screen\n",
      "2024-03-21 04:40:00 These Stocks Moved the Most Today: Chipotle, Boeing, PDD, Mobileye, Signet, Riot Platforms, BioNTech, and More\n"
     ]
    }
   ],
   "source": [
    "for i in news:\n",
    "    title = i['title']\n",
    "    date = datetime.fromtimestamp(i['providerPublishTime'])\n",
    "    print(date, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aca030fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = get_news_data(url=news[0]['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "318b29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "special_characters = \"!@#$%^&*()-+?_=,<>\\\"\"\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66fc7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = word_tokenize(article.lower())\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d5f8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_words = [word for word in lemmatized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90626178",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_words = [word for word in lemmatized_words if word not in special_characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84d00f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = \" \".join(preprocessed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "105835c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
    "\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5be23844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'neutral', 'score': 0.9990010857582092}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_pipe(out + out + out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1fc71aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "903"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8857f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nvidia',\n",
       " 'nasdaq',\n",
       " ':',\n",
       " 'nvda',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'best-performing',\n",
       " 'stock',\n",
       " 'globally',\n",
       " 'over',\n",
       " 'the',\n",
       " 'past',\n",
       " '18',\n",
       " 'month',\n",
       " '.',\n",
       " 'the',\n",
       " 'chip',\n",
       " 'manufacturer',\n",
       " '’',\n",
       " 's',\n",
       " 'share',\n",
       " 'are',\n",
       " 'up',\n",
       " 'nearly',\n",
       " '7x',\n",
       " 'over',\n",
       " 'the',\n",
       " 'period',\n",
       " '.',\n",
       " 'this',\n",
       " 'extraordinary',\n",
       " 'growth',\n",
       " 'can',\n",
       " 'certainly',\n",
       " 'turn',\n",
       " 'some',\n",
       " 'investor',\n",
       " 'off',\n",
       " 'making',\n",
       " 'them',\n",
       " 'believe',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'overvalued',\n",
       " '.',\n",
       " 'however',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'worth',\n",
       " 'remembering',\n",
       " 'that',\n",
       " 'momentum',\n",
       " 'can',\n",
       " 'actually',\n",
       " 'be',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'best',\n",
       " 'indicator',\n",
       " 'of',\n",
       " 'forward',\n",
       " 'stock',\n",
       " 'performance',\n",
       " 'especially',\n",
       " 'if',\n",
       " 'the',\n",
       " 'company',\n",
       " 'ha',\n",
       " 'a',\n",
       " 'track',\n",
       " 'record',\n",
       " 'of',\n",
       " 'beating',\n",
       " 'expectation',\n",
       " '.',\n",
       " 'personally',\n",
       " 'i',\n",
       " 'remain',\n",
       " 'bullish',\n",
       " 'on',\n",
       " 'nvda',\n",
       " 'stock',\n",
       " 'not',\n",
       " 'just',\n",
       " 'because',\n",
       " 'of',\n",
       " 'momentum',\n",
       " 'but',\n",
       " 'because',\n",
       " 'the',\n",
       " 'company',\n",
       " 'is',\n",
       " 'so',\n",
       " 'central',\n",
       " 'to',\n",
       " 'the',\n",
       " 'ai',\n",
       " 'revolution',\n",
       " 'which',\n",
       " 'ha',\n",
       " 'only',\n",
       " 'just',\n",
       " 'begun',\n",
       " '.',\n",
       " 'nvidia',\n",
       " 'a',\n",
       " 'a',\n",
       " 'company',\n",
       " 'is',\n",
       " 'at',\n",
       " 'the',\n",
       " 'very',\n",
       " 'heart',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ai',\n",
       " 'revolution',\n",
       " 'due',\n",
       " 'to',\n",
       " 'it',\n",
       " 'graphic',\n",
       " 'processing',\n",
       " 'unit',\n",
       " 'gpus',\n",
       " 'which',\n",
       " 'posse',\n",
       " 'the',\n",
       " 'capability',\n",
       " 'required',\n",
       " 'for',\n",
       " 'huge',\n",
       " 'ai',\n",
       " 'and',\n",
       " 'large',\n",
       " 'language',\n",
       " 'model',\n",
       " '.',\n",
       " 'the',\n",
       " 'unit',\n",
       " 'were',\n",
       " 'originally',\n",
       " 'built',\n",
       " 'for',\n",
       " 'the',\n",
       " 'gaming',\n",
       " 'sector',\n",
       " 'but',\n",
       " 'gpus',\n",
       " 'are',\n",
       " 'also',\n",
       " 'perfect',\n",
       " 'for',\n",
       " 'ai',\n",
       " '’',\n",
       " 's',\n",
       " 'massive',\n",
       " 'data',\n",
       " 'processing',\n",
       " 'need',\n",
       " '.',\n",
       " 'unlike',\n",
       " 'central',\n",
       " 'processing',\n",
       " 'unit',\n",
       " 'cpu',\n",
       " 'that',\n",
       " 'handle',\n",
       " 'task',\n",
       " 'one',\n",
       " 'by',\n",
       " 'one',\n",
       " 'gpus',\n",
       " 'excel',\n",
       " 'at',\n",
       " 'parallel',\n",
       " 'processing',\n",
       " 'allowing',\n",
       " 'them',\n",
       " 'to',\n",
       " 'take',\n",
       " 'on',\n",
       " 'multiple',\n",
       " 'task',\n",
       " 'simultaneously',\n",
       " '.',\n",
       " 'without',\n",
       " 'this',\n",
       " 'technology',\n",
       " 'the',\n",
       " 'step',\n",
       " 'forward',\n",
       " 'we',\n",
       " '’',\n",
       " 've',\n",
       " 'seen',\n",
       " 'in',\n",
       " 'ai',\n",
       " 'which',\n",
       " 'includes',\n",
       " 'development',\n",
       " 'in',\n",
       " 'facial',\n",
       " 'recognition',\n",
       " 'technology',\n",
       " 'and',\n",
       " 'self-driving',\n",
       " 'car',\n",
       " 'wouldn',\n",
       " '’',\n",
       " 't',\n",
       " 'be',\n",
       " 'possible',\n",
       " '.',\n",
       " 'nvidia',\n",
       " '’',\n",
       " 's',\n",
       " 'dominance',\n",
       " 'stem',\n",
       " 'from',\n",
       " 'the',\n",
       " 'architecture',\n",
       " 'of',\n",
       " 'it',\n",
       " 'gpu',\n",
       " '.',\n",
       " 'unlike',\n",
       " 'cpu',\n",
       " 'with',\n",
       " 'a',\n",
       " 'few',\n",
       " 'core',\n",
       " 'nvidia',\n",
       " 'pack',\n",
       " 'a',\n",
       " 'massive',\n",
       " 'number',\n",
       " 'of',\n",
       " 'core',\n",
       " 'onto',\n",
       " 'a',\n",
       " 'single',\n",
       " 'chip',\n",
       " '.',\n",
       " 'in',\n",
       " 'turn',\n",
       " 'this',\n",
       " 'allows',\n",
       " 'for',\n",
       " 'high',\n",
       " 'processing',\n",
       " 'power',\n",
       " 'within',\n",
       " 'a',\n",
       " 'smaller',\n",
       " 'space',\n",
       " 'and',\n",
       " 'this',\n",
       " 'is',\n",
       " 'hugely',\n",
       " 'important',\n",
       " 'for',\n",
       " 'efficient',\n",
       " 'ai',\n",
       " 'processing',\n",
       " '.',\n",
       " 'moreover',\n",
       " 'nvidia',\n",
       " 'ha',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'high-bandwidth',\n",
       " 'memory',\n",
       " 'which',\n",
       " 'allows',\n",
       " 'these',\n",
       " 'core',\n",
       " 'to',\n",
       " 'access',\n",
       " 'data',\n",
       " 'rapidly',\n",
       " 'further',\n",
       " 'accelerating',\n",
       " 'ai',\n",
       " 'computation',\n",
       " '.',\n",
       " 'a',\n",
       " 'such',\n",
       " 'nvidia',\n",
       " 'ha',\n",
       " 'earned',\n",
       " 'a',\n",
       " 'significant',\n",
       " 'edge',\n",
       " 'in',\n",
       " 'the',\n",
       " 'ai',\n",
       " 'hardware',\n",
       " 'race',\n",
       " '.',\n",
       " 'however',\n",
       " 'in',\n",
       " 'the',\n",
       " 'ai',\n",
       " 'world',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'not',\n",
       " 'just',\n",
       " 'about',\n",
       " 'hardware',\n",
       " '.',\n",
       " 'nvidia',\n",
       " '’',\n",
       " 's',\n",
       " 'cuda',\n",
       " 'software',\n",
       " 'provides',\n",
       " 'direct',\n",
       " 'access',\n",
       " 'to',\n",
       " 'the',\n",
       " 'gpu',\n",
       " '’',\n",
       " 's',\n",
       " 'virtual',\n",
       " 'instruction',\n",
       " '.',\n",
       " 'this',\n",
       " 'software',\n",
       " 'ecosystem',\n",
       " 'empowers',\n",
       " 'developer',\n",
       " 'to',\n",
       " 'build',\n",
       " 'and',\n",
       " 'refine',\n",
       " 'ai',\n",
       " 'project',\n",
       " 'and',\n",
       " 'ha',\n",
       " 'made',\n",
       " 'nvidia',\n",
       " 'a',\n",
       " 'one-stop',\n",
       " 'shop',\n",
       " 'for',\n",
       " 'all',\n",
       " 'thing',\n",
       " 'ai',\n",
       " '.',\n",
       " 'nvidia',\n",
       " 'stock',\n",
       " 'is',\n",
       " 'expensive',\n",
       " 'in',\n",
       " 'that',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'becoming',\n",
       " 'le',\n",
       " 'affordable',\n",
       " 'for',\n",
       " 'many',\n",
       " 'investor',\n",
       " '.',\n",
       " 'trading',\n",
       " 'around',\n",
       " '900',\n",
       " 'a',\n",
       " 'share',\n",
       " 'some',\n",
       " 'investor',\n",
       " 'may',\n",
       " 'struggle',\n",
       " 'to',\n",
       " 'purchase',\n",
       " 'a',\n",
       " 'single',\n",
       " 'nvidia',\n",
       " 'share',\n",
       " 'a',\n",
       " 'part',\n",
       " 'of',\n",
       " 'a',\n",
       " 'diverse',\n",
       " 'portfolio',\n",
       " 'of',\n",
       " 'holding',\n",
       " '.',\n",
       " 'however',\n",
       " 'from',\n",
       " 'a',\n",
       " 'valuation',\n",
       " 'perspective',\n",
       " 'i',\n",
       " 'don',\n",
       " '’',\n",
       " 't',\n",
       " 'think',\n",
       " 'nvidia',\n",
       " 'stock',\n",
       " 'is',\n",
       " 'expensive',\n",
       " 'or',\n",
       " 'overpriced',\n",
       " '.',\n",
       " 'in',\n",
       " 'fact',\n",
       " 'it',\n",
       " 'may',\n",
       " 'still',\n",
       " 'represent',\n",
       " 'good',\n",
       " 'value',\n",
       " '.',\n",
       " 'nvidia',\n",
       " 'currently',\n",
       " 'trade',\n",
       " 'at',\n",
       " '35.4x',\n",
       " 'forward',\n",
       " 'earnings',\n",
       " 'making',\n",
       " 'it',\n",
       " 'more',\n",
       " 'expensive',\n",
       " 'than',\n",
       " 'the',\n",
       " 's',\n",
       " 'p',\n",
       " '500',\n",
       " 'spx',\n",
       " 'but',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'by',\n",
       " 'no',\n",
       " 'mean',\n",
       " 'too',\n",
       " 'expensive',\n",
       " 'for',\n",
       " 'the',\n",
       " 'tech',\n",
       " 'sector',\n",
       " '.',\n",
       " 'moreover',\n",
       " 'the',\n",
       " 'company',\n",
       " 'is',\n",
       " 'expected',\n",
       " 'to',\n",
       " 'continue',\n",
       " 'delivering',\n",
       " 'stellar',\n",
       " 'growth',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'medium',\n",
       " 'term',\n",
       " '.',\n",
       " 'in',\n",
       " 'fact',\n",
       " 'nvidia',\n",
       " '’',\n",
       " 's',\n",
       " 'earnings',\n",
       " 'are',\n",
       " 'forecasted',\n",
       " 'to',\n",
       " 'grow',\n",
       " 'by',\n",
       " '34.78',\n",
       " 'annually',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'medium',\n",
       " 'term',\n",
       " '.',\n",
       " 'this',\n",
       " 'mean',\n",
       " 'that',\n",
       " 'nvidia',\n",
       " '’',\n",
       " 's',\n",
       " 'all-important',\n",
       " 'peg',\n",
       " 'ratio',\n",
       " 'is',\n",
       " '1.02.',\n",
       " 'while',\n",
       " '1.0',\n",
       " 'may',\n",
       " 'be',\n",
       " 'considered',\n",
       " 'the',\n",
       " 'benchmark',\n",
       " 'for',\n",
       " 'fair',\n",
       " 'value',\n",
       " 'i',\n",
       " 'still',\n",
       " 'think',\n",
       " 'this',\n",
       " 'represents',\n",
       " 'good',\n",
       " 'value',\n",
       " 'noting',\n",
       " 'long-term',\n",
       " 'trend',\n",
       " 'in',\n",
       " 'the',\n",
       " 'ai',\n",
       " 'industry',\n",
       " 'and',\n",
       " 'the',\n",
       " 'market',\n",
       " '’',\n",
       " 's',\n",
       " 'bullishness',\n",
       " 'on',\n",
       " 'u.s.',\n",
       " 'tech',\n",
       " '.',\n",
       " 'in',\n",
       " 'turn',\n",
       " 'this',\n",
       " 'mean',\n",
       " 'that',\n",
       " 'nvidia',\n",
       " 'is',\n",
       " 'trading',\n",
       " 'at',\n",
       " '29.77x',\n",
       " 'earnings',\n",
       " 'for',\n",
       " '2026',\n",
       " '25.26x',\n",
       " 'earnings',\n",
       " 'for',\n",
       " '2027',\n",
       " 'and',\n",
       " '21.49x',\n",
       " 'earnings',\n",
       " 'for',\n",
       " '2028.',\n",
       " 'moreover',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'worth',\n",
       " 'recognizing',\n",
       " 'that',\n",
       " 'nvidia',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'on',\n",
       " 'beating',\n",
       " 'analyst',\n",
       " '’',\n",
       " 'most',\n",
       " 'bullish',\n",
       " 'forecast',\n",
       " '.',\n",
       " 'that',\n",
       " '’',\n",
       " 's',\n",
       " 'always',\n",
       " 'a',\n",
       " 'good',\n",
       " 'sign',\n",
       " 'and',\n",
       " 'maybe',\n",
       " 'it',\n",
       " 'could',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'beat',\n",
       " 'expectation',\n",
       " 'going',\n",
       " 'forward',\n",
       " '.',\n",
       " 'due',\n",
       " 'to',\n",
       " 'it',\n",
       " 'enabling',\n",
       " 'position',\n",
       " 'in',\n",
       " 'the',\n",
       " 'ai',\n",
       " 'revolution',\n",
       " 'and',\n",
       " 'it',\n",
       " 'attractive',\n",
       " 'valuation',\n",
       " 'metric',\n",
       " 'nvidia',\n",
       " 'stock',\n",
       " 'earns',\n",
       " 'a',\n",
       " 'strong',\n",
       " 'buy',\n",
       " 'from',\n",
       " 'analyst',\n",
       " '.',\n",
       " 'currently',\n",
       " 'nvidia',\n",
       " 'ha',\n",
       " '39',\n",
       " 'buy',\n",
       " 'two',\n",
       " 'hold',\n",
       " 'rating',\n",
       " 'and',\n",
       " 'zero',\n",
       " 'sell',\n",
       " 'rating',\n",
       " '.',\n",
       " 'the',\n",
       " 'average',\n",
       " 'nvidia',\n",
       " 'stock',\n",
       " 'target',\n",
       " 'price',\n",
       " 'is',\n",
       " '913.74',\n",
       " 'inferring',\n",
       " '1.1',\n",
       " 'upside',\n",
       " 'potential',\n",
       " '.',\n",
       " 'the',\n",
       " 'highest',\n",
       " 'share',\n",
       " 'price',\n",
       " 'target',\n",
       " 'is',\n",
       " '1,200',\n",
       " 'and',\n",
       " 'the',\n",
       " 'lowest',\n",
       " 'share',\n",
       " 'price',\n",
       " 'target',\n",
       " 'is',\n",
       " '608.40',\n",
       " '.',\n",
       " 'nvidia',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'central',\n",
       " 'to',\n",
       " 'the',\n",
       " 'ai',\n",
       " 'revolution',\n",
       " 'but',\n",
       " 'there',\n",
       " 'are',\n",
       " 'two',\n",
       " 'important',\n",
       " 'thing',\n",
       " 'to',\n",
       " 'consider',\n",
       " 'moving',\n",
       " 'forward',\n",
       " ':',\n",
       " 'nvidia',\n",
       " '’',\n",
       " 's',\n",
       " 'competitive',\n",
       " 'advantage',\n",
       " 'in',\n",
       " 'the',\n",
       " 'all-important',\n",
       " 'generative',\n",
       " 'ai',\n",
       " 'market',\n",
       " 'and',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'the',\n",
       " 'ai',\n",
       " 'revolution',\n",
       " 'ha',\n",
       " 'only',\n",
       " 'just',\n",
       " 'begun',\n",
       " '.',\n",
       " 'over',\n",
       " 'the',\n",
       " 'past',\n",
       " '18',\n",
       " 'month',\n",
       " 'nvidia',\n",
       " 'ha',\n",
       " 'found',\n",
       " 'itself',\n",
       " 'with',\n",
       " 'a',\n",
       " 'formidable',\n",
       " 'moat',\n",
       " 'which',\n",
       " 'it',\n",
       " 'ha',\n",
       " 'successfully',\n",
       " 'built',\n",
       " 'upon',\n",
       " '.',\n",
       " 'other',\n",
       " 'company',\n",
       " 'including',\n",
       " 'intel',\n",
       " 'nasdaq',\n",
       " ':',\n",
       " 'intc',\n",
       " 'have',\n",
       " 'eye',\n",
       " 'on',\n",
       " 'nvidia',\n",
       " '’',\n",
       " 's',\n",
       " 'crown',\n",
       " 'but',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'not',\n",
       " 'clear',\n",
       " 'how',\n",
       " 'they',\n",
       " 'will',\n",
       " 'catch',\n",
       " 'up',\n",
       " '.',\n",
       " 'the',\n",
       " 'santa',\n",
       " 'clara-based',\n",
       " 'firm',\n",
       " '’',\n",
       " 's',\n",
       " 'new',\n",
       " 'h200',\n",
       " 'chipset',\n",
       " 'is',\n",
       " 'the',\n",
       " 'must-have',\n",
       " 'for',\n",
       " 'generative',\n",
       " 'ai',\n",
       " 'and',\n",
       " 'large',\n",
       " 'language',\n",
       " 'model',\n",
       " '.',\n",
       " 'the',\n",
       " 'h200',\n",
       " 'is',\n",
       " 'thought',\n",
       " 'to',\n",
       " 'be',\n",
       " 'between',\n",
       " '1.4',\n",
       " 'and',\n",
       " '1.9',\n",
       " 'time',\n",
       " 'faster',\n",
       " 'than',\n",
       " 'the',\n",
       " 'h100',\n",
       " 'when',\n",
       " 'it',\n",
       " 'come',\n",
       " 'to',\n",
       " 'large',\n",
       " 'language',\n",
       " 'model',\n",
       " 'inference',\n",
       " '.',\n",
       " 'that',\n",
       " '’',\n",
       " 's',\n",
       " 'an',\n",
       " 'impressive',\n",
       " 'jump',\n",
       " 'in',\n",
       " 'just',\n",
       " 'one',\n",
       " 'year',\n",
       " '.',\n",
       " 'moreover',\n",
       " 'the',\n",
       " 'market',\n",
       " 'is',\n",
       " 'growing',\n",
       " 'and',\n",
       " 'ha',\n",
       " 'the',\n",
       " 'potential',\n",
       " 'to',\n",
       " 'grow',\n",
       " 'much',\n",
       " 'faster',\n",
       " '.',\n",
       " 'softbank',\n",
       " '’',\n",
       " 's',\n",
       " 'otc',\n",
       " ':',\n",
       " 'sftby',\n",
       " 'masayoshi',\n",
       " 'son',\n",
       " 'is',\n",
       " 'considering',\n",
       " 'a',\n",
       " '100',\n",
       " 'billion',\n",
       " 'venture',\n",
       " 'in',\n",
       " 'the',\n",
       " 'ai',\n",
       " 'chip',\n",
       " 'space',\n",
       " 'and',\n",
       " 'openai',\n",
       " '’',\n",
       " 's',\n",
       " 'sam',\n",
       " 'altman',\n",
       " 'is',\n",
       " 'reportedly',\n",
       " 'looking',\n",
       " 'for',\n",
       " '7',\n",
       " 'trillion',\n",
       " 'for',\n",
       " 'a',\n",
       " 'string',\n",
       " 'of',\n",
       " 'ai',\n",
       " 'chip',\n",
       " 'factory',\n",
       " 'that',\n",
       " 'would',\n",
       " 'respond',\n",
       " 'to',\n",
       " 'burgeoning',\n",
       " 'demand',\n",
       " 'and',\n",
       " 'restructure',\n",
       " 'the',\n",
       " 'world',\n",
       " '’',\n",
       " 's',\n",
       " 'semiconductor',\n",
       " 'sector',\n",
       " '.',\n",
       " 'given',\n",
       " 'the',\n",
       " 'near-term',\n",
       " 'momentum',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sector',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'demand',\n",
       " 'for',\n",
       " 'gpus',\n",
       " 'still',\n",
       " 'outstrips',\n",
       " 'supply',\n",
       " 'and',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'we',\n",
       " 'really',\n",
       " 'are',\n",
       " 'just',\n",
       " 'at',\n",
       " 'the',\n",
       " 'start',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ai',\n",
       " 'revolution',\n",
       " 'i',\n",
       " 'remain',\n",
       " 'bullish',\n",
       " 'on',\n",
       " 'nvidia',\n",
       " '.',\n",
       " 'disclosure']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0ea7ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nvidia nasdaq : nvda ha been one of the best-performing stock globally over the past 18 month . the chip manufacturer ’ s share are up nearly 7x over the period . this extraordinary growth can certainly turn some investor off making them believe it ’ s overvalued . however it ’ s worth remembering that momentum can actually be one of the best indicator of forward stock performance especially if the company ha a track record of beating expectation . personally i remain bullish on nvda stock not just because of momentum but because the company is so central to the ai revolution which ha only just begun . nvidia a a company is at the very heart of the ai revolution due to it graphic processing unit gpus which posse the capability required for huge ai and large language model . the unit were originally built for the gaming sector but gpus are also perfect for ai ’ s massive data processing need . unlike central processing unit cpu that handle task one by one gpus excel at parallel processing allowing them to take on multiple task simultaneously . without this technology the step forward we ’ ve seen in ai which includes development in facial recognition technology and self-driving car wouldn ’ t be possible . nvidia ’ s dominance stem from the architecture of it gpu . unlike cpu with a few core nvidia pack a massive number of core onto a single chip . in turn this allows for high processing power within a smaller space and this is hugely important for efficient ai processing . moreover nvidia ha focused on high-bandwidth memory which allows these core to access data rapidly further accelerating ai computation . a such nvidia ha earned a significant edge in the ai hardware race . however in the ai world it ’ s not just about hardware . nvidia ’ s cuda software provides direct access to the gpu ’ s virtual instruction . this software ecosystem empowers developer to build and refine ai project and ha made nvidia a one-stop shop for all thing ai . nvidia stock is expensive in that it ’ s becoming le affordable for many investor . trading around 900 a share some investor may struggle to purchase a single nvidia share a part of a diverse portfolio of holding . however from a valuation perspective i don ’ t think nvidia stock is expensive or overpriced . in fact it may still represent good value . nvidia currently trade at 35.4x forward earnings making it more expensive than the s p 500 spx but it ’ s by no mean too expensive for the tech sector . moreover the company is expected to continue delivering stellar growth throughout the medium term . in fact nvidia ’ s earnings are forecasted to grow by 34.78 annually throughout the medium term . this mean that nvidia ’ s all-important peg ratio is 1.02. while 1.0 may be considered the benchmark for fair value i still think this represents good value noting long-term trend in the ai industry and the market ’ s bullishness on u.s. tech . in turn this mean that nvidia is trading at 29.77x earnings for 2026 25.26x earnings for 2027 and 21.49x earnings for 2028. moreover it ’ s worth recognizing that nvidia just keep on beating analyst ’ most bullish forecast . that ’ s always a good sign and maybe it could continue to beat expectation going forward . due to it enabling position in the ai revolution and it attractive valuation metric nvidia stock earns a strong buy from analyst . currently nvidia ha 39 buy two hold rating and zero sell rating . the average nvidia stock target price is 913.74 inferring 1.1 upside potential . the highest share price target is 1,200 and the lowest share price target is 608.40 . nvidia ha been central to the ai revolution but there are two important thing to consider moving forward : nvidia ’ s competitive advantage in the all-important generative ai market and the fact that the ai revolution ha only just begun . over the past 18 month nvidia ha found itself with a formidable moat which it ha successfully built upon . other company including intel nasdaq : intc have eye on nvidia ’ s crown but it ’ s not clear how they will catch up . the santa clara-based firm ’ s new h200 chipset is the must-have for generative ai and large language model . the h200 is thought to be between 1.4 and 1.9 time faster than the h100 when it come to large language model inference . that ’ s an impressive jump in just one year . moreover the market is growing and ha the potential to grow much faster . softbank ’ s otc : sftby masayoshi son is considering a 100 billion venture in the ai chip space and openai ’ s sam altman is reportedly looking for 7 trillion for a string of ai chip factory that would respond to burgeoning demand and restructure the world ’ s semiconductor sector . given the near-term momentum in the sector the fact that demand for gpus still outstrips supply and the fact that we really are just at the start of the ai revolution i remain bullish on nvidia . disclosure'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf64ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
