{
    "title": null,
    "date": "2/6/2024",
    "url": "https://techhq.com/2024/02/the-ai-chips-race-is-about-to-get-intense-with-metas-artemis/",
    "text": "Dashveenjit Kaur @DashveenjitK dashveen@hybrid.co Mark Zuckerberg, CEO of Meta, testifies during the US Senate Judiciary Committee hearing, \u201cBig Tech and the Online Child Sexual Exploitation Crisis,\u201d in Washington, DC, on January 31, 2024. (Photo by Brendan SMIALOWSKI/AFP). A whirlwind of generative AI innovation in the past year alone has exposed major tech companies\u2019 profound reliance on Nvidia.\u00a0Crafting chatbots\u00a0and other AI products has become an intricate dance with specialized chips\u00a0largely made by Nvidia in the preceding years. Pouring billions of dollars into Nvidia\u2019s systems, the tech behemoths have found themselves straining against the chipmaker\u2019s inability to keep pace with the soaring demand. Faced with this problem,\u00a0industry titans like Amazon, Google, Meta, and Microsoft are trying to seize control of their fate by forging their own AI chips. READ NEXTMore of Meta\u2019s missteps released After all, in-house chips would enable the giants to steer the course of their own destiny, slashing costs, eradicating chip shortages, and envisioning a future where they offer these cutting-edge chips to businesses tethered to their cloud services -creating their own silicon fiefdoms, rather than being entirely dependent on the likes of Nvidia (and potentially AMD and Intel). The most recent tech giant to announce plans to go solo is Meta, which is rumored to be developing a new AI chip, \u201cArtemis,\u201d set for release later this year. The chip, designed to complement the\u00a0extensive array of Nvidia H100\u00a0chips recently acquired by Meta, aligns with the company\u2019s strategic focus on inference\u2014the crucial decision-making facet of AI. While bearing similarities to the previously announced MTIA chip, which surfaced last year, Artemis seems to emphasize inference over training AI models. H100 Tensor Core GPU. Source: Nvidia. However, it is worth noting that Meta is entering the AI chip arena at a point when competition has gained momentum. It started with a significant move last July, when Meta disrupted the competition for advanced AI by unveiling Llama 2, a model akin to the one driving ChatGPT. Then, last month, Zuckerberg introduced his vision for artificial general intelligence (AGI) in an Instagram Reels video. In the previous earnings call, Zuckerberg also emphasized Meta\u2019s\u00a0substantial investment in AI, declaring it as the primary focus for 2024. In its quest to empower generative AI products across platforms like Facebook, Instagram, WhatsApp, and hardware devices like Ray-Ban smart glasses, the world\u2019s largest social media company is racing to enhance its computing capacity. Therefore, Meta is investing billions to build specialized chip arsenals and adapt data centers. Last Thursday,\u00a0Reuters\u00a0got hold of an internal company document that states that the parent company of Facebook intends to roll out an updated version of its custom chip into its data centers this year. The latest iteration of the custom chip, codenamed \u2018Artemis,\u2019 is designed to bolster the company\u2019s AI initiatives and might lessen its dependence on Nvidia chips, which presently hold a dominant position in the market. Mark Zuckerberg, CEO of Meta, testifies before the Senate Judiciary Committee on January 31, 2024 in Washington, DC. (Photo by Anna Moneymaker/GETTY IMAGES NORTH AMERICA/Getty Images via AFP). If successfully deployed at Meta\u2019s massive scale, an in-house semiconductor could trim annual energy costs by hundreds of millions of dollars, and slash billions in chip procurement expenses, suggests Dylan Patel, founder of silicon research group\u00a0SemiAnalysis. The deployment of Meta\u2019s chip would also mark a positive shift for its in-house AI silicon project. READ NEXTMeta in the hotseat\u2026 again In 2022, executives abandoned the initial chip version, choosing instead to invest billions in Nvidia\u2019s GPUs, dominant in AI training. The upside of that strategy is that Meta is poised to accumulate many coveted semiconductors. Mark Zuckerberg revealed to The Verge\u00a0that by the close of 2024, the tech giant will possess over 340,000 Nvidia H100 GPUs \u2013 the primary chips used by entities for training and deploying AI models like ChatGPT. Additionally, Zuckerberg anticipates Meta\u2019s collection to reach 600,000 GPUs by the year\u2019s end,\u00a0encompassing Nvidia\u2019s A100s and other\u00a0AI chips. The new AI chip by Meta follows its predecessor\u2019s ability for inference\u2014utilizing algorithms for ranking judgments and user prompt responses. Last year,\u00a0Reuters\u00a0reported\u00a0that Meta is also working on a more ambitious chip that, like GPUs, could perform training and inference. Zuckerberg also detailed Meta\u2019s strategy to\u00a0vie with Alphabet\u00a0and Microsoft in the high-stakes AI race. Meta aims to capitalize on its\u00a0extensive walled garden of data, highlighting the abundance of publicly shared images and videos on its platform and distinguishing it from competitors relying on web-crawled data. Beyond the existing generative AI, Zuckerberg envisions achieving \u201cgeneral intelligence,\u201d aspiring to develop top-tier AI products, including a world-class assistant for enhanced productivity.  Dashveenjit Kaur @DashveenjitK dashveen@hybrid.co 7 March 2024 7 March 2024 6 March 2024    "
}