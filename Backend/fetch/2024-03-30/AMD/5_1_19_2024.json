{
    "title": "GPU memory vulnerability could allow hackers to access LLM responses - and \nApple, Qualcomm, and AMD products ...",
    "date": "1/19/2024",
    "url": "https://www.itpro.com/security/gpu-memory-vulnerability-could-allow-hackers-to-access-llm-responses-and-apple-qualcomm-and-amd-products-were-all-at-risk",
    "text": "When you purchase through links on our site, we may earn an affiliate commission. Here\u2019s how it works. The GPU memory vulnerability could've left large language models wide open to exploitation by threat actors A GPU memory vulnerability dubbed \u2018LeftoverLocals\u2019 could expose LLM responses to hackers through leftover local memory, researchers have warned. Apple, Qualcomm, AMD, and Imagination are among the big name GPU vendors named as vulnerable according to research posted on the Trail of Bits blog. The GPUs have all been vulnerable to varying extents from as far back as September 2023, when researchers first began their investigation. Researchers were able to build a proof of concept (PoC) of the potential attack, citing an attached video in which they listen in to another user\u2019s interactive LLM session by recovering a GPUs local memory. From their tests, the researchers concluded that \u2018LeftoverLocals\u2019 could leak around 5.5mb per GPU invocation on an AMD Radeon RX 7900 XT, enough data to reconstruct an LLM with worryingly high precision.  Alarm raised over patched Phemedrone Stealer malware that's being used to target Windows PCs This malware is trying to steal your AWS keys and moreGitHub scrambles to rotate keys after credentials in production containers were potentially exposed With GPUs used extensively to support the high performance requirements of AI inferencing, this news is likely to raise serious concerns among enterprise AI users. Eleanor Watson, IEEE member and AI ethics engineer at Singularity university, told ITPro that although this particular vulnerability would require physical access to a GPU, the research highlights serious data protection risks. \u201cWhilst this particular type of exploit requires direct access to the GPU and its memory, it\u2019s indicative of the challenges of keeping our interactions with AI systems private,\u201d she said. Receive our latest news, industry updates, featured resources and more. Sign up today to receive our FREE report on AI cyber crime & security - newly updated for 2023. \u201cI expect that further vulnerabilities will be uncovered which are broadly applicable to a wide range of LLM systems, leading to a widespread doxxing of interactions and generations, and associated embarrassment,\u201d she added. To defend against the vulnerability, GPUs need a built in system to clear local memory between kernel calls. Some GPU vendors, including Nvidia and Intel, are already doing this, whereas others need to keep pace. In response to the news of \u2018LeftoverLocals\u2019, AMD stated plans to create a new mode that \u201cprevents processes from running in parallel on the GPU and clears local memory between processes on supported products\u201d, with an expected rollout of said mode by March 2024. \u201cThis mode would be designed to be set by an administrator and not enabled by default,\u201d the firm said in an advisory. \u201cSupporting documentation for the new mode, along with details of how to update AMD products, will be provided in a future update to this security notice.\u201d  Discover a data center revitalization strategy that will help you dominate \n\nDOWNLOAD NOW Apple responded to Trail of Bits but did not issue specific details of their patch, while Qualcomm issued a partial fix and Imagination a full patch in December. A failure to patch this vulnerability could see attackers targeting a variety of GPU applications and LLM sessions, including those within privacy-sensitive domains. Open source LLMs, however, are still the main target. Despite their ability to be rigorously audited, their reliance on closed-source GPUs means they are particularly susceptible to this attack. \u201cA lot of security hardening will need to be done on AI systems to make them more resistant to these kinds of vulnerabilities\u201d, Watson said. \u201cThis is a necessary component of the ongoing professionalization of AI, along with mastering challenges such as confabulation/hallucination in models\u201d, she added. George Fitzmaurice is a staff writer at ITPro, ChannelPro, and CloudPro, with a particular interest in AI regulation, data legislation, and market development. After graduating from the University of Oxford with a degree in English Language and Literature, he undertook an internship at the New Statesman before starting at ITPro. Outside of the office, George is both an aspiring musician and an avid reader. AI security tools promise to supercharge productivity, but experts worry cyber pros could become too reliant US offers $10 million bounty for information on UnitedHealth hackers Kaseya\u2019s big secret to be unveiled next month By Solomon KlappholzMarch 27, 2024 By George FitzmauriceMarch 27, 2024 By Solomon KlappholzMarch 27, 2024 By George FitzmauriceMarch 27, 2024 By Daniel ToddMarch 27, 2024 By Steve RangerMarch 27, 2024 By Daniel ToddMarch 27, 2024 By George FitzmauriceMarch 27, 2024 By Steve RangerMarch 26, 2024 By Solomon KlappholzMarch 26, 2024 By George FitzmauriceMarch 26, 2024 Posted Posted Posted Posted IT Pro is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site. \u00a9\nFuture US, Inc. Full 7th Floor, 130 West 42nd Street,\nNew York,\nNY 10036. "
}