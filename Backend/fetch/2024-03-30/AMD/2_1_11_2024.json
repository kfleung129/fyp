{
    "title": "Most formidable supercomputer ever is warming up for ChatGPT 5 \u2014 thousands \nof 'old' AMD GPU accelerators ...",
    "date": "1/11/2024",
    "url": "https://www.techradar.com/pro/most-formidable-supercomputer-ever-is-warming-up-for-chatgpt-5-thousands-of-old-amd-gpu-accelerators-crunched-1-trillion-parameter-models",
    "text": "When you purchase through links on our site, we may earn an affiliate commission. Here\u2019s how it works. Scientists trained a GPT-4-sized model using much fewer GPUs than you'd ordinarily need The most powerful supercomputer in the world has used just over 8% of the GPUs it's fitted with to train a large language model (LLM) containing one trillion parameters \u2013 comparable to OpenAI's GPT-4. Frontier, based in the Oak Ridge National Laboratory, used 3,072 of its AMD Radeon Instinct GPUs to train an AI system at the trillion-parameter scale, and it used 1,024 of these GPUs (roughly 2.5%) to train a 175-billion parameter model, essentially the same size as ChatGPT. The researchers needed 14TB RAM minimum to achieve these results, according to their paper, but each MI250X GPU only had 64GB VRAM, meaning the researchers had to group up several GPUs together.\u00a0This introduced another challenge in the form of parallelism, however, meaning the components had to communicate much better and more effectively as the overall size of the resources used to train the LLM increased. LLMs aren't typically trained on supercomputers, rather they're trained in specialized servers and require many more GPUs. ChatGPT, for example, was trained on more than 20,000 GPUs, according to TrendForce. But the researchers wanted to show whether they could train a supercomputer much quicker and more effectively way by harnessing various techniques made possible by the supercomputer architecture. The scientists used a combination of tensor parallelism \u2013 groups of GPUs sharing the parts of the same tensor \u2013 as well as pipeline parallelism \u2013 groups of GPUs hosting neighboring components. They also employed data parallelism to consume a large number of tokens simultaneously and a larger amount of computing resources. The overall effect was to achieve a much faster time. For the 22-billion parameter model, they achieved peak throughput of 38.38% (73.5 TFLOPS), 36.14% (69.2 TFLOPS) for the 175-billion parameter model, and 31.96% peak throughput (61.2 TFLOPS) for the 1-trillion parameter model. They also achieved 100% weak scaling efficiency%, as well as an 89.93% strong scaling performance for the 175-billion model, and an 87.05% strong scaling performance for the 1-trillion parameter model. Sign up to the TechRadar Pro newsletter to get all the top news, opinion, features and guidance your business needs to succeed! Although the researchers were open about the computing resources used, and the techniques involved, they neglected to mention the timescales involved in training an LLM in this way. TechRadar Pro asked the researchers for timings, but they have not responded at the time of writing. Keumars Afifi-Sabet is the Technology Editor for Live Science. He has written for a variety of publications including ITPro, The Week Digital and ComputerActive. He has worked as a technology journalist for more than five years, having previously held the role of features editor with ITPro. In his previous role, he oversaw the commissioning and publishing of long form in areas including AI, cyber security, cloud computing and digital transformation. Canonical announces Snap Store crackdown after crypto scam apps overload DNA storage is coming, it's just a matter of when not if \u2014 SNIA quietly unveils first specifications for storing bytes in DNA medium, an important first step towards almost ultra-cheap, limitless storage No, I don\u2019t want the iPhone 16 to go big on AI By Sead Fadilpa\u0161i\u0107March 29, 2024 By Sead Fadilpa\u0161i\u0107March 29, 2024 By Peter HaylesMarch 29, 2024 By Mackenzie FrazierMarch 29, 2024 By Sead Fadilpa\u0161i\u0107March 29, 2024 By Tom PowerMarch 29, 2024 By Mike MooreMarch 29, 2024 By Mackenzie FrazierMarch 29, 2024 By Lance UlanoffMarch 29, 2024 By Craig HaleMarch 29, 2024 By Craig HaleMarch 29, 2024 TechRadar is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site. \u00a9\nFuture US, Inc. Full 7th Floor, 130 West 42nd Street,\nNew York,\nNY 10036. "
}