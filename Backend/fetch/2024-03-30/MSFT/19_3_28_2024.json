{
    "title": "Microsoft intros new responsible AI tools in Azure Studio",
    "date": "3/28/2024",
    "url": "https://www.techtarget.com/searchenterpriseai/news/366575872/Microsoft-intros-new-responsible-AI-tools-in-Azure-Studio",
    "text": "Getty Images Microsoft introduced new responsible AI tools in Azure AI Studio aimed at reducing many of the hesitations enterprises have around generative AI systems. The company on Thursday introduced Prompt Shields, groundedness detection, safety system messages, safety evaluations, and risk and safety monitoring. The new tools come as the tech giant and its rival Google work to address challenges with their generative AI tools in recent months. For example, a Microsoft whistleblower wrote to the Federal Trade Commission detailing safety concerns about Microsoft Copilot Designer. Meanwhile, Google turned off the image-generating feature of its Gemini large language model after it generated biased images of key historical figures. Google also expanded its Search Generative Experience by allowing it to answer users' questions. However, there are reports that SGE's responses are spammy and prompt malware and scams. The new responsible AI tools address concerns of enterprises that are hesitant to use generative AI. \"One of the barriers to adoption of generative AI among enterprises today is trust -- a lack of trust in these systems,\" Forrester Research analyst Brandon Purcell said. Many enterprises are concerned about hallucinations, where an LLM or AI tool generates incorrect information, and the fact that the tools are susceptible to intellectual property leakage. \"Microsoft is ... releasing products that are hopefully going to help generate trust in the market,\" Purcell said. For example, the Prompt Shields feature detects and blocks prompt injection attacks. It is currently available in preview. Prompt injection is when a user with bad intentions tries to make the LLM do something it is not supposed to do, such as provide access to its training data or engage in hate speech or sexualized content. Another tool, groundedness detection, helps detect hallucinations in model outputs. That tool is coming soon. \"Reducing hallucinations is probably one of the main seemingly unsolvable challenges in adopting generative AI for mission-critical business use cases,\" Gartner analyst Jason Wong said. Since most language models tend to hallucinate, a tool that reduces hallucinations will be critical to enterprises. \"Groundedness detection should reduce the hallucination rate and give businesses confidence and trust that the system is working as it should,\" Purcell said. Microsoft's new responsible AI tools also show how the vendor is responding to some of the new regulations coming out of both the European Union and the U.S., according to Northeastern University AI policy adviser Michael Bennett. Earlier this month, the EU approved the EU AI Act. The act regulates AI systems that interact with humans in different industries, including education, employment and public systems. Thus, having these responsible AI safeguards eases the minds of enterprises conducting business in the EU, Bennett said. \"These types of safeguards will probably put those larger companies at greater ease, [but] not erase the concern altogether,\" he said. Enterprises will also feel comfortable using the systems in the U.S., where different state districts have introduced individual AI laws, Bennett added. However, despite vendors' safeguards, enterprises must perform their due diligence, Purcell said. \"No matter how many great features Microsoft or other companies roll out, a company that is using generative AI needs to have a stringent monitoring system in place to be able to detect when the model is not performing and leading to poor business outcomes,\" he said. Other responsible AI tools Microsoft introduced include safety system messages, safety evaluations, and risk and safety monitoring. Safety system messages, coming soon, steer the model's behavior toward safe outputs. Safety evaluations assess applications' vulnerability to jailbreak attacks; the tool is available in preview. Risk and safety monitoring understands what model inputs, outputs and end users trigger content filters. It is also available in preview. Esther Ajao is a TechTarget Editorial news writer and podcast host covering artificial intelligence software and systems. The data platform vendor's new language model was designed to provide open source users with AI development capabilities similar ... The BI vendor's latest innovations include conversational analytics capabilities and prebuilt models designed to help customers ... The longtime analytics vendor's new bot enables customers to embed emerging technology in third-party applications just as they ... The OMB's new policy calls for federal agencies to be transparent about AI use and designate chief AI officers to coordinate ... Use cases for the still-developing metaverse are growing as the technologies that enable this next iteration of the internet ... Cybersecurity and cloud top the list of 2024's tech investment drivers, according to an Enterprise Strategy Group survey. But ... The various types of database software come with advantages, limitations and optimal uses that prospective buyers should be aware... Fragmented data protection laws, technology disruptions, AI adoption, data governance and consumer trust are among the complex ... Any organization that wants to drive decision-making with data or use generative AI won't succeed without understanding how to ... Industrial metaverse use cases for manufacturing include facility design and employee training, though adoption of the technology... AI is being adopted for company supply chains, but organizations should guard against hype and assess the business value and ... Learn how the integration of AI and machine learning into manufacturing processes can help organizations meet quality control ... All Rights Reserved, \nCopyright 2018 - 2024, TechTarget\n\n\nPrivacy Policy\n\n\n\nCookie Preferences \n\n\n\nCookie Preferences \n\n\n\nDo Not Sell or Share My Personal Information "
}